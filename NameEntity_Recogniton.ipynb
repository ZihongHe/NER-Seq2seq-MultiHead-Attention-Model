{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NameEntity_Recogniton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvlyUjRTp2tW",
        "colab_type": "text"
      },
      "source": [
        "# **Parameters Setting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA4lZg44p6U4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### preprocess\n",
        "\n",
        "# true for document level, false for sentence level\n",
        "doc_level = False\n",
        "\n",
        "### embedding\n",
        "\n",
        "# choose embedding type\n",
        "whether_word2vec = True\n",
        "whether_POS_tag = True\n",
        "whether_tfidf = True\n",
        "\n",
        "# word2vec embedding and window size\n",
        "word2vec_size = 16\n",
        "word2vec_window = 9\n",
        "\n",
        "# Pos-tagging to vec embedding size and window size\n",
        "Postag2vec_size = 16\n",
        "Postag2vec_window = 9\n",
        "\n",
        "# Name Entity to vec gold label embedding size and window size\n",
        "NameEntity_embedding_size = 32\n",
        "NameEntity_window = 7\n",
        "\n",
        "### model\n",
        "\n",
        "## model architecture: \n",
        "\n",
        "# whether an dense after cancate(if exists, else attention layer) in the first Muti-head attention\n",
        "whether_multihead_dense_encoder = True\n",
        "\n",
        "# whether perform layer Nomalisation in model\n",
        "whether_layer_normal = False\n",
        "\n",
        "# whether to perform batch-normalisation \n",
        "whether_batch_normal = True\n",
        "\n",
        "\n",
        "# whether an dense after the first Muti-head attention in encoder\n",
        "whether_dense_encoder = False\n",
        "\n",
        "# whether a hidden layer after decoder LSTM output \n",
        "whether_dense_decoder = True\n",
        "\n",
        "# whether a hidden layer after concatenate layer\n",
        "whether_concate_dense = True\n",
        "\n",
        "# number of units in the hidden layer after concatenate layer\n",
        "num_concate_dense_unit = 64\n",
        "\n",
        "## attention and lstm dimension\n",
        "\n",
        "# whether scale dot product or normal dot product in attention layer\n",
        "whether_scale = True\n",
        "\n",
        "# lstm dimension of encoder and decoder\n",
        "lstm_hidden_dim = 32\n",
        "\n",
        "# bi-LSTM dimension of encoder\n",
        "bi_lstm_hidden_dim = 32\n",
        "\n",
        "### hyperparameters\n",
        "\n",
        "epoch = 800\n",
        "\n",
        "model_learning_rate = 0.0002\n",
        "\n",
        "weight_decay = 1e-5\n",
        "\n",
        "beta_1_value = 0.9"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euJcrHN--YPe",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocess**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCj16wCWDMxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this step is to define input level(sentence-level or document-level) and split sequence\n",
        "\n",
        "import csv\n",
        "# read in data\n",
        "data_train = list(csv.reader(open('train.csv')))[1:100000]\n",
        "data_test = list(csv.reader(open('test.csv')))[1:5000]\n",
        "\n",
        "\n",
        "# record only the split sentence\n",
        "sentence_token = []\n",
        "\n",
        "# split data for document-level sequence or sentence-level input\n",
        "\n",
        "#choose which column for the id\n",
        "if doc_level:\n",
        "  column_ = 1\n",
        "else:\n",
        "  column_ = 2\n",
        "\n",
        "data_train_copy = []\n",
        "data_test_copy = []\n",
        "\n",
        "sentence_doc = []\n",
        "name_entity_doc = []\n",
        "\n",
        "#  the current sentence or the document id\n",
        "current_id = int(data_train[0][column_])\n",
        "\n",
        "# reform data\n",
        "for i in range(len(data_train)):\n",
        "  if(int(data_train[i][column_]) != current_id and i!=0):\n",
        "    data_train_copy.append([sentence_doc, name_entity_doc])\n",
        "    sentence_token.append(sentence_doc)\n",
        "    sentence_doc = []\n",
        "    name_entity_doc = []\n",
        "    current_id+=1\n",
        "\n",
        "  sentence_doc.append(data_train[i][3])\n",
        "  name_entity_doc.append(data_train[i][4])\n",
        "\n",
        "sentence_doc = []\n",
        "\n",
        "#  the current sentence or the document id\n",
        "current_id = int(data_test[0][column_])\n",
        "\n",
        "for i in range(len(data_test)):\n",
        "  if(int(data_test[i][column_]) != current_id and i!=0):\n",
        "    data_test_copy.append([sentence_doc])\n",
        "    sentence_token.append(sentence_doc)\n",
        "    sentence_doc = []\n",
        "    current_id+=1\n",
        "\n",
        "  sentence_doc.append(data_test[i][3])\n",
        "\n",
        "data_train = data_train_copy\n",
        "data_test = data_test_copy"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE7bokiz4ygW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ae662b0c-fda1-4f45-920f-8b8fde040524"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "data_train, data_val = train_test_split(data_train, test_size=0.02, random_state = 0)\n",
        "print(\"length of train data:\", len(data_train))\n",
        "print(\"length of val data:\", len(data_val))\n",
        "print(\"length of test data:\", len(data_test))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of train data: 4144\n",
            "length of val data: 85\n",
            "length of test data: 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az6_zeJPveV6",
        "colab_type": "text"
      },
      "source": [
        "# **Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAmwNaJvTpxZ",
        "colab_type": "text"
      },
      "source": [
        "## **Word to vector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMn9KyiMFOD8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "71c3a329-2356-4d09-be65-bc12bc02cf71"
      },
      "source": [
        "# Word2Vec based on given words. Reference: https://radimrehurek.com/gensim/models/word2vec.html\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "word2vec_model = Word2Vec(np.asarray(sentence_token), size=word2vec_size,  window=word2vec_window, min_count=1, sg=1)\n",
        "\n",
        "data_train_wv = []\n",
        "data_val_wv = []\n",
        "data_test_wv = []\n",
        "\n",
        "for i in range(len(data_train)):\n",
        "  train_wv = []\n",
        "  for word in data_train[i][0]:\n",
        "    train_wv.append(word2vec_model[word])\n",
        "  data_train_wv.append(train_wv)\n",
        "\n",
        "for i in range(len(data_val)):\n",
        "  val_wv = []\n",
        "  for word in data_val[i][0]:\n",
        "    val_wv.append(word2vec_model[word])\n",
        "  data_val_wv.append(val_wv)\n",
        "\n",
        "for i in range(len(data_test)):\n",
        "  test_wv = []\n",
        "  for word in data_test[i][0]:\n",
        "    test_wv.append(word2vec_model[word])\n",
        "  data_test_wv.append(test_wv)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEfoPA7u76Vc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93309f4b-8c58-4650-dd64-2e9ff87bec9b"
      },
      "source": [
        "print(\"embedding size of word2vec:\", data_train_wv[0][0].shape)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding size of word2vec: (16,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtiUhBMYy4Q3",
        "colab_type": "text"
      },
      "source": [
        "## **TFIDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bCTA6pyy7H7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reference: http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/\n",
        "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "# each word is expressed as tfidf type in one-hot style after PCA\n",
        "# for example, index of \"apple\" is 3, vocaburary size is 1000, tfidf value of \"apple\" is 0.73, and thus, \"apple\" is expressed as 0,0,0.73,...0,0. Then it will be sent into PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import decomposition\n",
        "import numpy as np\n",
        "\n",
        "def origin(data): \n",
        "  return data\n",
        "\n",
        "# convert data into tfidf type\n",
        "tfidf_model = TfidfVectorizer(use_idf=True, lowercase= True, token_pattern=False, tokenizer=origin, preprocessor=origin)\n",
        "data_tfidf = tfidf_model.fit_transform(sentence_token).todense()\n",
        "data_tfidf = np.asarray(data_tfidf)\n",
        "tfidf_features = tfidf_model.get_feature_names()\n",
        "# dictioanry for tfidf\n",
        "tfidf_as_int = dict((element, i) for i, element in enumerate(tfidf_features))\n",
        "\n",
        "# create lists to record generated tfidf\n",
        "data_train_tfidf = []\n",
        "data_val_tfidf = []\n",
        "data_test_tfidf = []\n",
        "\n",
        "for i in range(len(data_train)):\n",
        "  train_tfidf = []\n",
        "  for j in range(len(data_train[i][0])):\n",
        "    tfidf_index = tfidf_as_int[data_train[i][0][j]]\n",
        "    train_tfidf.append(data_tfidf[i][tfidf_index])\n",
        "  data_train_tfidf.append(train_tfidf)\n",
        "\n",
        "for i in range(len(data_test)):\n",
        "  test_tfidf = []\n",
        "  for j in range(len(data_test[i][0])):\n",
        "    tfidf_index = tfidf_as_int[data_test[i][0][j]]\n",
        "    test_tfidf.append(data_tfidf[len(data_train) + i][tfidf_index])\n",
        "  data_test_tfidf.append(test_tfidf)\n",
        "\n",
        "for i in range(len(data_val)):\n",
        "  val_tfidf = []\n",
        "  for j in range(len(data_val[i][0])):\n",
        "    tfidf_index = tfidf_as_int[data_val[i][0][j]]\n",
        "    val_tfidf.append(data_tfidf[len(data_train) + len(data_test) + i][tfidf_index])\n",
        "  data_val_tfidf.append(val_tfidf)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFKGAWioU6EL",
        "colab_type": "text"
      },
      "source": [
        "## **POS-Tagging** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOEHljssUPiH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ee6a7078-66ee-40b4-8a8f-dd06604ca1f0"
      },
      "source": [
        "# record Pos-tagging in all sentences/docs\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perception_tagger')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "pos_tag_list = []\n",
        "\n",
        "for i in range(len(sentence_token)):\n",
        "  pos_tag = nltk.pos_tag(sentence_token[i])\n",
        "  tags = []\n",
        "  for tag in pos_tag:\n",
        "    tags.append(tag[1])\n",
        "  pos_tag_list.append(tags)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading averaged_perception_tagger: Package\n",
            "[nltk_data]     'averaged_perception_tagger' not found in index\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww1pvmhj2Ngo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "7979dd1b-b167-4751-da6c-677d47d057b7"
      },
      "source": [
        "# Word2Vec Model for Pos-tagging based on generated Pos-tagging. Reference: https://radimrehurek.com/gensim/models/word2vec.html\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "Postag_model = Word2Vec(np.asarray(pos_tag_list), size=Postag2vec_size,  window=Postag2vec_window, min_count=1,sg=1)\n",
        "\n",
        "# record Pos-tag in vector for words' expression\n",
        "data_train_postag = []\n",
        "data_val_postag = []\n",
        "data_test_postag = []\n",
        "\n",
        "for i in range(len(data_train)):\n",
        "  train_postag = []\n",
        "  pos_tag = nltk.pos_tag(data_train[i][0])\n",
        "  for word in pos_tag:\n",
        "    train_postag.append(Postag_model[word[1]])\n",
        "  data_train_postag.append(train_postag)\n",
        "\n",
        "for i in range(len(data_val)):\n",
        "  val_postag = []\n",
        "  pos_tag = nltk.pos_tag(data_val[i][0])\n",
        "  for word in pos_tag:\n",
        "    val_postag.append(Postag_model[word[1]])\n",
        "  data_val_postag.append(val_postag)\n",
        "\n",
        "for i in range(len(data_test)):\n",
        "  test_postag = []\n",
        "  pos_tag = nltk.pos_tag(data_test[i][0])\n",
        "  for word in pos_tag:\n",
        "    test_postag.append(Postag_model[word[1]])\n",
        "  data_test_postag.append(test_postag)\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuCtiWE67-yH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0885953e-bedc-4c66-d37f-6da56a27d0b4"
      },
      "source": [
        "print(\"Postag shape for train data:\", data_train_postag[0][0].shape)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Postag shape for train data: (16,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVam4M4EiR80",
        "colab_type": "text"
      },
      "source": [
        "## **Embedding Vector Concat**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8aaPLN9iZtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this cell is to perform concation to be the whole embedding vectors\n",
        "\n",
        "def embedding_concat(data_wv, data_postag, data_tfidf):\n",
        "  data_concat = []\n",
        "  for i in range(len(data_wv)):\n",
        "    data = []\n",
        "    for j in range(len(data_wv[i])):\n",
        "      embedding = []\n",
        "      # choose which type of embedding should be included in data\n",
        "      if whether_word2vec:\n",
        "        embedding += list(data_wv[i][j])\n",
        "      if whether_POS_tag:\n",
        "        embedding += list(data_postag[i][j])\n",
        "      if whether_tfidf:\n",
        "        embedding += [data_tfidf[i][j]]\n",
        "      data.append(embedding)\n",
        "    data_concat.append(data)\n",
        "\n",
        "  return data_concat\n",
        "\n",
        "embedding_train = embedding_concat(data_train_wv, data_train_postag, data_train_tfidf)\n",
        "embedding_val = embedding_concat(data_val_wv, data_val_postag, data_val_tfidf)\n",
        "embedding_test = embedding_concat(data_test_wv, data_test_postag, data_test_tfidf)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU8RkyJuTORk",
        "colab_type": "text"
      },
      "source": [
        "## **Embedding Sentence/doc as Encoder Input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo3m1QiXTBfr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6af6b921-dfab-43cf-8245-82ece330ff07"
      },
      "source": [
        "# Padding sentence/doc to ensure a same shape of data as encoder input of model\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "if doc_level:\n",
        "  \n",
        "  # 1920 is the longest document length , plus 1 to 1921 to satify label length\n",
        "  embedding_train = preprocessing.sequence.pad_sequences(embedding_train, maxlen=1921, dtype='float', padding='post', truncating='post', value=0)\n",
        "  embedding_val = preprocessing.sequence.pad_sequences(embedding_val, maxlen=1921, dtype='float', padding='post', truncating='post', value=0)\n",
        "\n",
        "else:\n",
        "  # 205 is the longest sentence length, plus 1 to 206 to satify label length\n",
        "  embedding_train = preprocessing.sequence.pad_sequences(embedding_train, maxlen=206, dtype='float', padding='post', truncating='post', value=0)\n",
        "  embedding_val = preprocessing.sequence.pad_sequences(embedding_val, maxlen=206, dtype='float', padding='post', truncating='post', value=0)\n",
        "\n",
        "embedding_test = preprocessing.sequence.pad_sequences(embedding_test, maxlen=None, dtype='float', padding='post', truncating='post', value=0)\n",
        "embedding_test = preprocessing.sequence.pad_sequences(embedding_test, maxlen=embedding_test.shape[1]+1, dtype='float', padding='post', truncating='post', value=0)\n",
        "\n",
        "\n",
        "print(\"shape of Encoder Input in train set:\", embedding_train.shape)\n",
        "print(\"shape of Encoder Input in validation set:\", embedding_val.shape)\n",
        "print(\"shape of Encoder Input in test set:\", embedding_test.shape)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of Encoder Input in train set: (4144, 206, 33)\n",
            "shape of Encoder Input in validation set: (85, 206, 33)\n",
            "shape of Encoder Input in test set: (200, 88, 33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS2S8JgvMNMc",
        "colab_type": "text"
      },
      "source": [
        "## **Name Entity as vectors(word2vec)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2SSTOqL_45W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "942c2dff-d8ba-4975-facf-92261079f0ad"
      },
      "source": [
        "# To ensure a better expression of Name Entity label in decoder input of the model below.\n",
        "# We establish word2vec model on Name Entity label based on given data's context\n",
        "\n",
        "\n",
        "labels = []\n",
        "for sentence in data_train:\n",
        "  # for the first decoder input, it should be a \"BOS\"(Begin of sequence)\n",
        "  labels.append([\"<BOS>\"] + sentence[1])\n",
        "for sentence in data_val:\n",
        "  labels.append([\"<BOS>\"] + sentence[1])\n",
        "\n",
        "#Word2Vec Model for Name entity label. Reference: https://radimrehurek.com/gensim/models/word2vec.html\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "Nameentity_model = Word2Vec(np.asarray(labels), size=NameEntity_embedding_size,  window=NameEntity_window, min_count=1,sg=1)\n",
        "\n",
        "train_label_vector = []\n",
        "val_label_vector = []\n",
        "\n",
        "BOS_vector = Nameentity_model[\"<BOS>\"]\n",
        "\n",
        "# record Name Entity gold label set in vector-type\n",
        "for i in range(len(data_train)):\n",
        "  train_label = []\n",
        "  train_label.append(BOS_vector)\n",
        "  for word in data_train[i][1]:\n",
        "    train_label.append(Nameentity_model[word])\n",
        "  train_label_vector.append(train_label)\n",
        "\n",
        "for i in range(len(data_val)):\n",
        "  val_label = []\n",
        "  val_label.append(BOS_vector)\n",
        "  for word in data_val[i][1]:\n",
        "    val_label.append(Nameentity_model[word])\n",
        "  val_label_vector.append(val_label)\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A77VerPPRmzC",
        "colab_type": "text"
      },
      "source": [
        "## **Name Entity Vectors as Decoder Input**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfUaEp9RVukr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e5b49df6-54df-4a50-c096-c286608ed46f"
      },
      "source": [
        "# padding word2vec vectors as decoder input\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "if doc_level:\n",
        "  train_decoder_input = preprocessing.sequence.pad_sequences(list(train_label_vector), maxlen = 1921, dtype='float', padding='post', truncating='post', value=0)\n",
        "  val_decoder_input = preprocessing.sequence.pad_sequences(list(val_label_vector), maxlen = 1921, dtype='float', padding='post', truncating='post', value=0)\n",
        "\n",
        "else:\n",
        "  train_decoder_input = preprocessing.sequence.pad_sequences(list(train_label_vector), maxlen = 206, dtype='float', padding='post', truncating='post', value=0)\n",
        "  val_decoder_input = preprocessing.sequence.pad_sequences(list(val_label_vector), maxlen = 206, dtype='float', padding='post', truncating='post', value=0)\n",
        "\n",
        "print(\"shape of decoder input in training set:\", train_decoder_input.shape)\n",
        "print(\"shape of decoder input in validation set:\", val_decoder_input.shape)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of decoder input in training set: (4144, 206, 32)\n",
            "shape of decoder input in validation set: (85, 206, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaAVRdhfoCcK",
        "colab_type": "text"
      },
      "source": [
        "## **Name Entity as category(one-hot)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLwveScQn9V9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fec609f-28e2-48f9-c844-e7797799435f"
      },
      "source": [
        "\n",
        "# dictionary building for labels to convert it into category\n",
        "# this step is performed since decoder output of attetion model below is one-hot type of Name entity label\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# collect all labels for dictionary building use\n",
        "labels = []\n",
        "for sentence in data_train:\n",
        "  labels += sentence[1]\n",
        "for sentence in data_val:\n",
        "  labels += sentence[1]\n",
        "\n",
        "# all unrepeated labels\n",
        "labels = sorted(list(set((labels))))\n",
        "\n",
        "# dictioanry for labels\n",
        "labels_as_int = dict((label, i) for i, label in enumerate(labels))\n",
        "int_as_labels = dict((i, label) for i, label in enumerate(labels))\n",
        "\n",
        "train_label_category = []\n",
        "val_label_category = []\n",
        "\n",
        "# convert labels into category\n",
        "for sentence in data_train:\n",
        "  label = [labels_as_int[i] for i in sentence[1]]\n",
        "  label = to_categorical(label, num_classes=len(labels_as_int))\n",
        "  train_label_category.append(label)\n",
        "for sentence in data_val:\n",
        "  label = [labels_as_int[i] for i in sentence[1]]\n",
        "  label = to_categorical(label, num_classes=len(labels_as_int))\n",
        "  val_label_category.append(label)\n",
        "\n",
        "print(\"Type number of Name entity is\", len(labels_as_int))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type number of Name entity is 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2i-5F-zoNH-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c4436a25-652b-43f6-84bd-40459a9f57d7"
      },
      "source": [
        "# Padding sentence to ensure a same shape of data as decoder output\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "if doc_level:\n",
        "  train_label_category = preprocessing.sequence.pad_sequences(train_label_category, maxlen=1920, dtype='float', padding='post', truncating='post', value=0)\n",
        "  val_label_category = preprocessing.sequence.pad_sequences(val_label_category, maxlen=1920, dtype='float', padding='post', truncating='post', value=0)\n",
        "else:\n",
        "  train_label_category = preprocessing.sequence.pad_sequences(train_label_category, maxlen=205, dtype='float', padding='post', truncating='post', value=0)\n",
        "  val_label_category = preprocessing.sequence.pad_sequences(val_label_category, maxlen=205, dtype='float', padding='post', truncating='post', value=0)\n",
        "\n",
        "print(\"shape of one-hot style for Name Entity labels in training set, after padding:\", train_label_category.shape)\n",
        "print(\"shape of one-hot style for Name Entity labels in validation set, after padding:\", val_label_category.shape)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of one-hot style for Name Entity labels in training set, after padding: (4144, 205, 3)\n",
            "shape of one-hot style for Name Entity labels in validation set, after padding: (85, 205, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KzdLZqRskXi",
        "colab_type": "text"
      },
      "source": [
        "## **Name Entity one-hot arrays as Decoder Output**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JWVkedPsp2K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ce2e19d-3e93-481f-8527-f8b555c5f974"
      },
      "source": [
        "# Input&output of train model for Seq2seq model include encoder input, decoder input and decoder output\n",
        "# Eecoder input is the input sentence's embedding vectors - embedding_train/embedding_val\n",
        "# the final Name Entity of a sentence has a all-zeros label. And thus ,no gradient for it.\n",
        "\n",
        "# Decoder output is to pad an array of 0 after label vector\n",
        "train_decoder_output = preprocessing.sequence.pad_sequences(train_label_category, maxlen = train_label_category.shape[1] + 1, dtype='float', padding='post', truncating='post', value=0)\n",
        "val_decoder_output = preprocessing.sequence.pad_sequences(val_label_category, maxlen = val_label_category.shape[1] + 1, dtype='float', padding='post', truncating='post', value=0)\n",
        "\n",
        "print(\"shape of decoder output in training set:\", train_decoder_output.shape)\n",
        "print(\"shape of decoder output in validation set:\", val_decoder_output.shape)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of decoder output in training set: (4144, 206, 3)\n",
            "shape of decoder output in validation set: (85, 206, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHdE-XQjrWF6",
        "colab_type": "text"
      },
      "source": [
        "# **Seq2seq Model with Multi-head Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxxgAhdS-YjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seq2seq Reference: https://keras.io/examples/lstm_seq2seq/\n",
        "# Dot product Attention layer Reference: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\n",
        "# Muti-head Attention structure Reference: https://www.tensorflow.org/tutorials/text/transformer\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Masking\n",
        "from tensorflow.keras import Model, optimizers\n",
        "from tensorflow.keras.layers import LSTM, Concatenate, Attention, Dropout, Bidirectional, BatchNormalization, LayerNormalization\n",
        "import tensorflow as tf"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1mUbYPXwkCk",
        "colab_type": "text"
      },
      "source": [
        "## **Model Structure Design** - LayerNormal Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvlVYzkT5lrX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the function for dense with add & norm\n",
        "\n",
        "def Layernormal(data, whether_layer_normal, whether_batch_normal):\n",
        "\n",
        "  data = Dropout(0.2)(data)\n",
        "  dense = Dense(data.get_shape().as_list()[2], activation='relu')\n",
        "  if whether_layer_normal:\n",
        "    data_copy = dense(data)\n",
        "    layernormalization = LayerNormalization()\n",
        "    data = layernormalization(data_copy+data)\n",
        "    if whether_batch_normal:\n",
        "      batchnormalization = BatchNormalization()\n",
        "      data = batchnormalization(data)\n",
        "      return data, dense, layernormalization, batchnormalization\n",
        "    return data, dense, layernormalization\n",
        "  else:\n",
        "    data = dense(data)\n",
        "    if whether_batch_normal:\n",
        "      batchnormalization = BatchNormalization()\n",
        "      data = batchnormalization(data)\n",
        "      return data, dense, batchnormalization\n",
        "    return data, dense\n",
        "\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9aqHToJ-hzp",
        "colab_type": "text"
      },
      "source": [
        "## **Model Structure Design** - Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMFbbVNgoqYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# unit number for LSTM has been declare in parameter setting\n",
        "\n",
        "##### Model Architecture setting\n",
        "\n",
        "#### encoder block\n",
        "\n",
        "## encoder input\n",
        "encoder_input = Input(shape = (None, embedding_train.shape[2]))\n",
        "\n",
        "## encoder Bi-lstm \n",
        "encoder_bilstm = Bidirectional(LSTM(bi_lstm_hidden_dim, return_state=False, return_sequences=True))\n",
        "encoder_bilstm_output = encoder_bilstm(encoder_input)\n",
        "\n",
        "## encoder lstm for multi-head attention\n",
        "encoder_LSTM_attention = LSTM(lstm_hidden_dim, return_state=True, return_sequences=True)\n",
        "encoder_lstm_output, encoder_h, encoder_c = encoder_LSTM_attention(encoder_input)\n",
        "\n",
        "## The first Multi-head attention in encoder\n",
        "\n",
        "# linear layer for heads\n",
        "encoder_dense_query1 = Dense(lstm_hidden_dim, activation='relu')\n",
        "encoder_dense_value1 = Dense(lstm_hidden_dim, activation='relu')\n",
        "encoder_dense_key1 = Dense(lstm_hidden_dim, activation='relu')\n",
        "\n",
        "encoder_query1 = encoder_dense_query1(encoder_lstm_output)\n",
        "encoder_value1 = encoder_dense_value1(encoder_lstm_output)\n",
        "encoder_key1 = encoder_dense_key1(encoder_lstm_output)\n",
        "\n",
        "# dot product attention\n",
        "Mutihead_attention1 = Attention(use_scale = whether_scale)\n",
        "encoder_Mutihead_attention_output1 = Mutihead_attention1([encoder_query1, encoder_value1, encoder_key1])\n",
        "\n",
        "# linear layer after concate in Multi-head Attention\n",
        "if whether_multihead_dense_encoder:\n",
        "  if whether_layer_normal:\n",
        "    if whether_batch_normal:\n",
        "      encoder_Mutihead_attention_output1, dense_multihead_attention, LayerNormalization1, BatchNormalization1 = Layernormal(encoder_Mutihead_attention_output1, whether_layer_normal, whether_batch_normal)\n",
        "    else:\n",
        "      encoder_Mutihead_attention_output1, dense_multihead_attention, LayerNormalization1 = Layernormal(encoder_Mutihead_attention_output1, whether_layer_normal, whether_batch_normal)\n",
        "  else:\n",
        "    if whether_batch_normal:\n",
        "      encoder_Mutihead_attention_output1, dense_multihead_attention, BatchNormalization1 = Layernormal(encoder_Mutihead_attention_output1, whether_layer_normal, whether_batch_normal)\n",
        "    else:\n",
        "      encoder_Mutihead_attention_output1, dense_multihead_attention = Layernormal(encoder_Mutihead_attention_output1, whether_layer_normal, whether_batch_normal)\n",
        "\n",
        "# whether a layer after the first Muti-head attention in encoder\n",
        "if whether_dense_encoder:\n",
        "  if whether_layer_normal:\n",
        "    if whether_batch_normal:\n",
        "      encoder_Mutihead_attention_output1, dense_encoder_relu, LayerNormalization2, BatchNormalization2 = Layernormal(encoder_Mutihead_attention_output1, whether_layer_normal, whether_batch_normal)\n",
        "    else:\n",
        "      encoder_Mutihead_attention_output1, dense_encoder_relu, LayerNormalization2 = Layernormal(encoder_Mutihead_attention_output1, whether_layer_normal, whether_batch_normal)  \n",
        "  else:\n",
        "    if whether_batch_normal:\n",
        "      encoder_Mutihead_attention_output1, dense_encoder_relu, BatchNormalization2 = Layernormal(encoder_Mutihead_attention_output1, whether_layer_normal, whether_batch_normal)\n",
        "    else:\n",
        "      encoder_Mutihead_attention_output1, dense_encoder_relu = Layernormal(encoder_Mutihead_attention_output1, whether_layer_normal, whether_batch_normal)  \n",
        "\n",
        "\n",
        "## second Muti-head Attention Encoder part - Key and value \n",
        "encoder_dense_value2 = Dense(lstm_hidden_dim, activation='relu')\n",
        "encoder_dense_key2 = Dense(lstm_hidden_dim, activation='relu')\n",
        "\n",
        "encoder_value2 = encoder_dense_value2(encoder_Mutihead_attention_output1)\n",
        "encoder_key2 = encoder_dense_key2(encoder_Mutihead_attention_output1)\n"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZPrSfnN-5xs",
        "colab_type": "text"
      },
      "source": [
        "## **Model Structure Design** - Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9eYbmtC-3sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### decoder basic block\n",
        "\n",
        "## decoder input\n",
        "decoder_input = Input(shape = (None, train_decoder_input.shape[2]))\n",
        "\n",
        "## decoder lstm\n",
        "lstm_decoder = LSTM(lstm_hidden_dim, return_state=True, return_sequences=True)\n",
        "decoder_lstm_query, decoder_h, decoder_c = lstm_decoder(decoder_input, initial_state=[encoder_h, encoder_c])\n",
        "\n",
        "# dense after deocder LSTM\n",
        "if whether_dense_decoder:\n",
        "  if whether_layer_normal:\n",
        "    if whether_batch_normal:\n",
        "      decoder_lstm_query, dense_decoder_relu, LayerNormalization3, BatchNormalization3 = Layernormal(decoder_lstm_query, whether_layer_normal, whether_batch_normal)\n",
        "    else:\n",
        "      decoder_lstm_query, dense_decoder_relu, LayerNormalization3 = Layernormal(decoder_lstm_query, whether_layer_normal, whether_batch_normal)\n",
        "  else:\n",
        "    if whether_batch_normal:\n",
        "      decoder_lstm_query, dense_decoder_relu, BatchNormalization3 = Layernormal(decoder_lstm_query, whether_layer_normal, whether_batch_normal)\n",
        "    else:\n",
        "      decoder_lstm_query, dense_decoder_relu = Layernormal(decoder_lstm_query, whether_layer_normal, whether_batch_normal)\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "## second Muti-head Attention Decoder part - Query in Decoder\n",
        "Mutihead_attention2 = Attention(use_scale = whether_scale)\n",
        "encoder_Mutihead_attention_output2 = Mutihead_attention2([decoder_lstm_query, encoder_value2, encoder_key2])\n",
        "\n",
        "#### concat encoder Bidirectional LSTM output and Attention output\n",
        "concat_layer = Concatenate(axis = 2)\n",
        "concat_output = concat_layer([encoder_bilstm_output, encoder_Mutihead_attention_output2, decoder_lstm_query])\n",
        "\n",
        "### hidden layer after concatenate layer\n",
        "if whether_concate_dense:\n",
        "  concat_output = Dropout(0.2)(concat_output)\n",
        "  Concate_dense = Dense(num_concate_dense_unit)\n",
        "  concat_output = Concate_dense(concat_output)\n",
        "  if whether_batch_normal:\n",
        "    BatchNormalization4 = BatchNormalization()\n",
        "    concat_output = BatchNormalization4(concat_output)\n",
        "\n",
        "#### decoder output block\n",
        "\n",
        "dense_decoder = Dense(train_decoder_output.shape[2], activation='softmax')\n",
        "decoder_output = dense_decoder(concat_output)\n"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMWetW2G_B9r",
        "colab_type": "text"
      },
      "source": [
        "## **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwzz5EPe-_Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Model_train = Model([encoder_input, decoder_input], decoder_output)\n",
        "\n",
        "#### define optimizer (Adam this is a useful optimizer based on our experience)\n",
        "adam_optimizer = optimizers.Adam(lr=model_learning_rate, beta_1 = beta_1_value, decay=weight_decay)\n",
        "\n",
        "#### model complile\n",
        "Model_train.compile(loss='categorical_crossentropy', optimizer=adam_optimizer, metrics=['mse'])"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIB15IUpwwI-",
        "colab_type": "text"
      },
      "source": [
        "## **End-to-end Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTNg04Pk4q7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c7a493a-bded-4dbb-e797-6a8169592a84"
      },
      "source": [
        "# Tensorflow with GPU for colab Reference: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ\n",
        "# model training/fitting (end-to-end learning)\n",
        "\n",
        "import tensorflow as tf\n",
        "try:\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    print(\"GPU device performed for model fitting\")\n",
        "    history = Model_train.fit([embedding_train, train_decoder_input], train_decoder_output, epochs=epoch, batch_size=3000, validation_data=([embedding_val, val_decoder_input], val_decoder_output))\n",
        "except:\n",
        "  history = Model_train.fit([embedding_train, train_decoder_input], train_decoder_output, epochs=epoch, batch_size=3000, validation_data=([embedding_val, val_decoder_input], val_decoder_output))\n",
        "\n",
        "Model_train.save(\"Model_train.hdf5\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU device performed for model fitting\n",
            "Epoch 1/800\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.5520 - mse: 0.1724 - val_loss: 0.1273 - val_mse: 0.1247\n",
            "Epoch 2/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.4500 - mse: 0.1632 - val_loss: 0.1206 - val_mse: 0.1232\n",
            "Epoch 3/800\n",
            "2/2 [==============================] - 1s 259ms/step - loss: 0.3596 - mse: 0.1543 - val_loss: 0.1141 - val_mse: 0.1217\n",
            "Epoch 4/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.2822 - mse: 0.1461 - val_loss: 0.1080 - val_mse: 0.1202\n",
            "Epoch 5/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.2181 - mse: 0.1397 - val_loss: 0.1022 - val_mse: 0.1188\n",
            "Epoch 6/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.1690 - mse: 0.1352 - val_loss: 0.0968 - val_mse: 0.1175\n",
            "Epoch 7/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.1326 - mse: 0.1321 - val_loss: 0.0919 - val_mse: 0.1163\n",
            "Epoch 8/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.1062 - mse: 0.1302 - val_loss: 0.0874 - val_mse: 0.1152\n",
            "Epoch 9/800\n",
            "2/2 [==============================] - 1s 259ms/step - loss: 0.0849 - mse: 0.1288 - val_loss: 0.0833 - val_mse: 0.1142\n",
            "Epoch 10/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0705 - mse: 0.1282 - val_loss: 0.0796 - val_mse: 0.1133\n",
            "Epoch 11/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0594 - mse: 0.1277 - val_loss: 0.0763 - val_mse: 0.1125\n",
            "Epoch 12/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0511 - mse: 0.1274 - val_loss: 0.0733 - val_mse: 0.1118\n",
            "Epoch 13/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0441 - mse: 0.1272 - val_loss: 0.0707 - val_mse: 0.1111\n",
            "Epoch 14/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0392 - mse: 0.1271 - val_loss: 0.0683 - val_mse: 0.1106\n",
            "Epoch 15/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0357 - mse: 0.1271 - val_loss: 0.0662 - val_mse: 0.1101\n",
            "Epoch 16/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0324 - mse: 0.1271 - val_loss: 0.0643 - val_mse: 0.1097\n",
            "Epoch 17/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0297 - mse: 0.1270 - val_loss: 0.0626 - val_mse: 0.1093\n",
            "Epoch 18/800\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.0278 - mse: 0.1269 - val_loss: 0.0612 - val_mse: 0.1090\n",
            "Epoch 19/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0258 - mse: 0.1269 - val_loss: 0.0599 - val_mse: 0.1087\n",
            "Epoch 20/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0248 - mse: 0.1267 - val_loss: 0.0587 - val_mse: 0.1084\n",
            "Epoch 21/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0235 - mse: 0.1265 - val_loss: 0.0577 - val_mse: 0.1082\n",
            "Epoch 22/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0225 - mse: 0.1263 - val_loss: 0.0568 - val_mse: 0.1081\n",
            "Epoch 23/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0223 - mse: 0.1262 - val_loss: 0.0559 - val_mse: 0.1079\n",
            "Epoch 24/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0213 - mse: 0.1260 - val_loss: 0.0552 - val_mse: 0.1078\n",
            "Epoch 25/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0209 - mse: 0.1257 - val_loss: 0.0546 - val_mse: 0.1077\n",
            "Epoch 26/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0206 - mse: 0.1256 - val_loss: 0.0540 - val_mse: 0.1076\n",
            "Epoch 27/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0200 - mse: 0.1254 - val_loss: 0.0535 - val_mse: 0.1075\n",
            "Epoch 28/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0198 - mse: 0.1251 - val_loss: 0.0530 - val_mse: 0.1075\n",
            "Epoch 29/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0195 - mse: 0.1248 - val_loss: 0.0525 - val_mse: 0.1074\n",
            "Epoch 30/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0194 - mse: 0.1246 - val_loss: 0.0521 - val_mse: 0.1074\n",
            "Epoch 31/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0193 - mse: 0.1244 - val_loss: 0.0518 - val_mse: 0.1073\n",
            "Epoch 32/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0188 - mse: 0.1241 - val_loss: 0.0514 - val_mse: 0.1073\n",
            "Epoch 33/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0185 - mse: 0.1239 - val_loss: 0.0511 - val_mse: 0.1073\n",
            "Epoch 34/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0183 - mse: 0.1238 - val_loss: 0.0508 - val_mse: 0.1073\n",
            "Epoch 35/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0182 - mse: 0.1236 - val_loss: 0.0505 - val_mse: 0.1073\n",
            "Epoch 36/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0182 - mse: 0.1234 - val_loss: 0.0502 - val_mse: 0.1073\n",
            "Epoch 37/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0182 - mse: 0.1232 - val_loss: 0.0499 - val_mse: 0.1073\n",
            "Epoch 38/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0178 - mse: 0.1230 - val_loss: 0.0496 - val_mse: 0.1073\n",
            "Epoch 39/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0177 - mse: 0.1229 - val_loss: 0.0493 - val_mse: 0.1073\n",
            "Epoch 40/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0176 - mse: 0.1228 - val_loss: 0.0491 - val_mse: 0.1073\n",
            "Epoch 41/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0175 - mse: 0.1227 - val_loss: 0.0488 - val_mse: 0.1073\n",
            "Epoch 42/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0172 - mse: 0.1226 - val_loss: 0.0485 - val_mse: 0.1073\n",
            "Epoch 43/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0171 - mse: 0.1224 - val_loss: 0.0483 - val_mse: 0.1073\n",
            "Epoch 44/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0171 - mse: 0.1224 - val_loss: 0.0480 - val_mse: 0.1073\n",
            "Epoch 45/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0171 - mse: 0.1223 - val_loss: 0.0477 - val_mse: 0.1074\n",
            "Epoch 46/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0169 - mse: 0.1222 - val_loss: 0.0475 - val_mse: 0.1074\n",
            "Epoch 47/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0168 - mse: 0.1221 - val_loss: 0.0472 - val_mse: 0.1074\n",
            "Epoch 48/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0166 - mse: 0.1220 - val_loss: 0.0470 - val_mse: 0.1074\n",
            "Epoch 49/800\n",
            "2/2 [==============================] - 1s 285ms/step - loss: 0.0168 - mse: 0.1219 - val_loss: 0.0468 - val_mse: 0.1074\n",
            "Epoch 50/800\n",
            "2/2 [==============================] - 1s 283ms/step - loss: 0.0165 - mse: 0.1218 - val_loss: 0.0465 - val_mse: 0.1075\n",
            "Epoch 51/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0163 - mse: 0.1218 - val_loss: 0.0463 - val_mse: 0.1075\n",
            "Epoch 52/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0163 - mse: 0.1217 - val_loss: 0.0460 - val_mse: 0.1075\n",
            "Epoch 53/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0166 - mse: 0.1217 - val_loss: 0.0458 - val_mse: 0.1076\n",
            "Epoch 54/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0162 - mse: 0.1216 - val_loss: 0.0455 - val_mse: 0.1076\n",
            "Epoch 55/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0163 - mse: 0.1216 - val_loss: 0.0453 - val_mse: 0.1076\n",
            "Epoch 56/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0160 - mse: 0.1215 - val_loss: 0.0450 - val_mse: 0.1077\n",
            "Epoch 57/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0161 - mse: 0.1214 - val_loss: 0.0448 - val_mse: 0.1077\n",
            "Epoch 58/800\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.0159 - mse: 0.1214 - val_loss: 0.0445 - val_mse: 0.1078\n",
            "Epoch 59/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0158 - mse: 0.1213 - val_loss: 0.0443 - val_mse: 0.1078\n",
            "Epoch 60/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0158 - mse: 0.1213 - val_loss: 0.0440 - val_mse: 0.1078\n",
            "Epoch 61/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0159 - mse: 0.1213 - val_loss: 0.0438 - val_mse: 0.1079\n",
            "Epoch 62/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0156 - mse: 0.1212 - val_loss: 0.0436 - val_mse: 0.1079\n",
            "Epoch 63/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0156 - mse: 0.1212 - val_loss: 0.0433 - val_mse: 0.1080\n",
            "Epoch 64/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0157 - mse: 0.1212 - val_loss: 0.0431 - val_mse: 0.1080\n",
            "Epoch 65/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0154 - mse: 0.1211 - val_loss: 0.0429 - val_mse: 0.1081\n",
            "Epoch 66/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0152 - mse: 0.1211 - val_loss: 0.0426 - val_mse: 0.1082\n",
            "Epoch 67/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0152 - mse: 0.1211 - val_loss: 0.0424 - val_mse: 0.1082\n",
            "Epoch 68/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0153 - mse: 0.1211 - val_loss: 0.0421 - val_mse: 0.1083\n",
            "Epoch 69/800\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.0153 - mse: 0.1210 - val_loss: 0.0419 - val_mse: 0.1083\n",
            "Epoch 70/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0150 - mse: 0.1210 - val_loss: 0.0416 - val_mse: 0.1084\n",
            "Epoch 71/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0153 - mse: 0.1210 - val_loss: 0.0414 - val_mse: 0.1084\n",
            "Epoch 72/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0150 - mse: 0.1210 - val_loss: 0.0412 - val_mse: 0.1085\n",
            "Epoch 73/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0151 - mse: 0.1210 - val_loss: 0.0409 - val_mse: 0.1086\n",
            "Epoch 74/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0149 - mse: 0.1209 - val_loss: 0.0407 - val_mse: 0.1086\n",
            "Epoch 75/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0148 - mse: 0.1209 - val_loss: 0.0405 - val_mse: 0.1087\n",
            "Epoch 76/800\n",
            "2/2 [==============================] - 1s 257ms/step - loss: 0.0149 - mse: 0.1209 - val_loss: 0.0403 - val_mse: 0.1087\n",
            "Epoch 77/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0147 - mse: 0.1208 - val_loss: 0.0400 - val_mse: 0.1088\n",
            "Epoch 78/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0145 - mse: 0.1209 - val_loss: 0.0398 - val_mse: 0.1089\n",
            "Epoch 79/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0147 - mse: 0.1208 - val_loss: 0.0396 - val_mse: 0.1089\n",
            "Epoch 80/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0145 - mse: 0.1208 - val_loss: 0.0394 - val_mse: 0.1090\n",
            "Epoch 81/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0145 - mse: 0.1208 - val_loss: 0.0391 - val_mse: 0.1091\n",
            "Epoch 82/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0143 - mse: 0.1208 - val_loss: 0.0389 - val_mse: 0.1091\n",
            "Epoch 83/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0144 - mse: 0.1208 - val_loss: 0.0387 - val_mse: 0.1092\n",
            "Epoch 84/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0145 - mse: 0.1207 - val_loss: 0.0385 - val_mse: 0.1093\n",
            "Epoch 85/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0143 - mse: 0.1207 - val_loss: 0.0383 - val_mse: 0.1094\n",
            "Epoch 86/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0145 - mse: 0.1207 - val_loss: 0.0380 - val_mse: 0.1094\n",
            "Epoch 87/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0142 - mse: 0.1207 - val_loss: 0.0378 - val_mse: 0.1095\n",
            "Epoch 88/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0143 - mse: 0.1207 - val_loss: 0.0376 - val_mse: 0.1096\n",
            "Epoch 89/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0143 - mse: 0.1207 - val_loss: 0.0374 - val_mse: 0.1097\n",
            "Epoch 90/800\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.0142 - mse: 0.1206 - val_loss: 0.0372 - val_mse: 0.1097\n",
            "Epoch 91/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0139 - mse: 0.1206 - val_loss: 0.0370 - val_mse: 0.1098\n",
            "Epoch 92/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0142 - mse: 0.1207 - val_loss: 0.0368 - val_mse: 0.1099\n",
            "Epoch 93/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0141 - mse: 0.1206 - val_loss: 0.0366 - val_mse: 0.1100\n",
            "Epoch 94/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0139 - mse: 0.1207 - val_loss: 0.0364 - val_mse: 0.1100\n",
            "Epoch 95/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0139 - mse: 0.1206 - val_loss: 0.0362 - val_mse: 0.1101\n",
            "Epoch 96/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0140 - mse: 0.1206 - val_loss: 0.0360 - val_mse: 0.1102\n",
            "Epoch 97/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0137 - mse: 0.1206 - val_loss: 0.0358 - val_mse: 0.1103\n",
            "Epoch 98/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0139 - mse: 0.1206 - val_loss: 0.0356 - val_mse: 0.1103\n",
            "Epoch 99/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0137 - mse: 0.1206 - val_loss: 0.0354 - val_mse: 0.1104\n",
            "Epoch 100/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0137 - mse: 0.1206 - val_loss: 0.0352 - val_mse: 0.1105\n",
            "Epoch 101/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0136 - mse: 0.1206 - val_loss: 0.0350 - val_mse: 0.1105\n",
            "Epoch 102/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0137 - mse: 0.1207 - val_loss: 0.0348 - val_mse: 0.1106\n",
            "Epoch 103/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0137 - mse: 0.1207 - val_loss: 0.0346 - val_mse: 0.1107\n",
            "Epoch 104/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0136 - mse: 0.1207 - val_loss: 0.0344 - val_mse: 0.1108\n",
            "Epoch 105/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0134 - mse: 0.1206 - val_loss: 0.0342 - val_mse: 0.1108\n",
            "Epoch 106/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0136 - mse: 0.1207 - val_loss: 0.0340 - val_mse: 0.1109\n",
            "Epoch 107/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0135 - mse: 0.1206 - val_loss: 0.0339 - val_mse: 0.1110\n",
            "Epoch 108/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0131 - mse: 0.1206 - val_loss: 0.0337 - val_mse: 0.1111\n",
            "Epoch 109/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0134 - mse: 0.1206 - val_loss: 0.0335 - val_mse: 0.1111\n",
            "Epoch 110/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0134 - mse: 0.1207 - val_loss: 0.0334 - val_mse: 0.1112\n",
            "Epoch 111/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0133 - mse: 0.1207 - val_loss: 0.0332 - val_mse: 0.1113\n",
            "Epoch 112/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0132 - mse: 0.1207 - val_loss: 0.0330 - val_mse: 0.1114\n",
            "Epoch 113/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0132 - mse: 0.1207 - val_loss: 0.0329 - val_mse: 0.1114\n",
            "Epoch 114/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0131 - mse: 0.1206 - val_loss: 0.0327 - val_mse: 0.1115\n",
            "Epoch 115/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0132 - mse: 0.1207 - val_loss: 0.0325 - val_mse: 0.1116\n",
            "Epoch 116/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0131 - mse: 0.1207 - val_loss: 0.0323 - val_mse: 0.1117\n",
            "Epoch 117/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0133 - mse: 0.1207 - val_loss: 0.0322 - val_mse: 0.1118\n",
            "Epoch 118/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0131 - mse: 0.1206 - val_loss: 0.0320 - val_mse: 0.1118\n",
            "Epoch 119/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0130 - mse: 0.1207 - val_loss: 0.0319 - val_mse: 0.1119\n",
            "Epoch 120/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0128 - mse: 0.1206 - val_loss: 0.0317 - val_mse: 0.1120\n",
            "Epoch 121/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0129 - mse: 0.1207 - val_loss: 0.0315 - val_mse: 0.1121\n",
            "Epoch 122/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0128 - mse: 0.1207 - val_loss: 0.0314 - val_mse: 0.1121\n",
            "Epoch 123/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0129 - mse: 0.1208 - val_loss: 0.0312 - val_mse: 0.1122\n",
            "Epoch 124/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0129 - mse: 0.1207 - val_loss: 0.0310 - val_mse: 0.1123\n",
            "Epoch 125/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0129 - mse: 0.1207 - val_loss: 0.0308 - val_mse: 0.1123\n",
            "Epoch 126/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0127 - mse: 0.1207 - val_loss: 0.0307 - val_mse: 0.1124\n",
            "Epoch 127/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0128 - mse: 0.1207 - val_loss: 0.0305 - val_mse: 0.1125\n",
            "Epoch 128/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0127 - mse: 0.1208 - val_loss: 0.0304 - val_mse: 0.1125\n",
            "Epoch 129/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0127 - mse: 0.1208 - val_loss: 0.0302 - val_mse: 0.1126\n",
            "Epoch 130/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0127 - mse: 0.1208 - val_loss: 0.0301 - val_mse: 0.1127\n",
            "Epoch 131/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0127 - mse: 0.1207 - val_loss: 0.0299 - val_mse: 0.1128\n",
            "Epoch 132/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0127 - mse: 0.1208 - val_loss: 0.0297 - val_mse: 0.1128\n",
            "Epoch 133/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0125 - mse: 0.1207 - val_loss: 0.0296 - val_mse: 0.1129\n",
            "Epoch 134/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0124 - mse: 0.1207 - val_loss: 0.0294 - val_mse: 0.1130\n",
            "Epoch 135/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0125 - mse: 0.1208 - val_loss: 0.0293 - val_mse: 0.1130\n",
            "Epoch 136/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0126 - mse: 0.1208 - val_loss: 0.0291 - val_mse: 0.1131\n",
            "Epoch 137/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0124 - mse: 0.1208 - val_loss: 0.0289 - val_mse: 0.1131\n",
            "Epoch 138/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0125 - mse: 0.1208 - val_loss: 0.0288 - val_mse: 0.1132\n",
            "Epoch 139/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0125 - mse: 0.1209 - val_loss: 0.0286 - val_mse: 0.1133\n",
            "Epoch 140/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0124 - mse: 0.1209 - val_loss: 0.0284 - val_mse: 0.1133\n",
            "Epoch 141/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0123 - mse: 0.1209 - val_loss: 0.0283 - val_mse: 0.1134\n",
            "Epoch 142/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0123 - mse: 0.1209 - val_loss: 0.0281 - val_mse: 0.1134\n",
            "Epoch 143/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0123 - mse: 0.1209 - val_loss: 0.0280 - val_mse: 0.1135\n",
            "Epoch 144/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0122 - mse: 0.1209 - val_loss: 0.0278 - val_mse: 0.1135\n",
            "Epoch 145/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0122 - mse: 0.1209 - val_loss: 0.0277 - val_mse: 0.1136\n",
            "Epoch 146/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0122 - mse: 0.1209 - val_loss: 0.0275 - val_mse: 0.1136\n",
            "Epoch 147/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0121 - mse: 0.1209 - val_loss: 0.0274 - val_mse: 0.1137\n",
            "Epoch 148/800\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.0121 - mse: 0.1210 - val_loss: 0.0272 - val_mse: 0.1137\n",
            "Epoch 149/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0122 - mse: 0.1210 - val_loss: 0.0271 - val_mse: 0.1138\n",
            "Epoch 150/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0121 - mse: 0.1210 - val_loss: 0.0269 - val_mse: 0.1138\n",
            "Epoch 151/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0120 - mse: 0.1210 - val_loss: 0.0267 - val_mse: 0.1139\n",
            "Epoch 152/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0121 - mse: 0.1211 - val_loss: 0.0266 - val_mse: 0.1139\n",
            "Epoch 153/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0120 - mse: 0.1211 - val_loss: 0.0264 - val_mse: 0.1140\n",
            "Epoch 154/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0120 - mse: 0.1211 - val_loss: 0.0263 - val_mse: 0.1140\n",
            "Epoch 155/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0119 - mse: 0.1212 - val_loss: 0.0261 - val_mse: 0.1141\n",
            "Epoch 156/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0119 - mse: 0.1211 - val_loss: 0.0260 - val_mse: 0.1141\n",
            "Epoch 157/800\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.0119 - mse: 0.1212 - val_loss: 0.0258 - val_mse: 0.1141\n",
            "Epoch 158/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0119 - mse: 0.1212 - val_loss: 0.0257 - val_mse: 0.1142\n",
            "Epoch 159/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0118 - mse: 0.1212 - val_loss: 0.0255 - val_mse: 0.1142\n",
            "Epoch 160/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0118 - mse: 0.1213 - val_loss: 0.0254 - val_mse: 0.1143\n",
            "Epoch 161/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0118 - mse: 0.1213 - val_loss: 0.0252 - val_mse: 0.1143\n",
            "Epoch 162/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0116 - mse: 0.1213 - val_loss: 0.0251 - val_mse: 0.1143\n",
            "Epoch 163/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0118 - mse: 0.1214 - val_loss: 0.0249 - val_mse: 0.1144\n",
            "Epoch 164/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0117 - mse: 0.1213 - val_loss: 0.0247 - val_mse: 0.1144\n",
            "Epoch 165/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0116 - mse: 0.1213 - val_loss: 0.0245 - val_mse: 0.1144\n",
            "Epoch 166/800\n",
            "2/2 [==============================] - 1s 283ms/step - loss: 0.0115 - mse: 0.1213 - val_loss: 0.0244 - val_mse: 0.1145\n",
            "Epoch 167/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0116 - mse: 0.1214 - val_loss: 0.0242 - val_mse: 0.1145\n",
            "Epoch 168/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0116 - mse: 0.1214 - val_loss: 0.0241 - val_mse: 0.1145\n",
            "Epoch 169/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0116 - mse: 0.1215 - val_loss: 0.0239 - val_mse: 0.1146\n",
            "Epoch 170/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0115 - mse: 0.1215 - val_loss: 0.0237 - val_mse: 0.1146\n",
            "Epoch 171/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0116 - mse: 0.1215 - val_loss: 0.0236 - val_mse: 0.1146\n",
            "Epoch 172/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0114 - mse: 0.1215 - val_loss: 0.0235 - val_mse: 0.1147\n",
            "Epoch 173/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0115 - mse: 0.1215 - val_loss: 0.0233 - val_mse: 0.1147\n",
            "Epoch 174/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0114 - mse: 0.1216 - val_loss: 0.0232 - val_mse: 0.1147\n",
            "Epoch 175/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0114 - mse: 0.1216 - val_loss: 0.0230 - val_mse: 0.1148\n",
            "Epoch 176/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0114 - mse: 0.1217 - val_loss: 0.0229 - val_mse: 0.1148\n",
            "Epoch 177/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0113 - mse: 0.1217 - val_loss: 0.0228 - val_mse: 0.1149\n",
            "Epoch 178/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0114 - mse: 0.1216 - val_loss: 0.0227 - val_mse: 0.1149\n",
            "Epoch 179/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0113 - mse: 0.1217 - val_loss: 0.0226 - val_mse: 0.1149\n",
            "Epoch 180/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0113 - mse: 0.1217 - val_loss: 0.0225 - val_mse: 0.1150\n",
            "Epoch 181/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0114 - mse: 0.1217 - val_loss: 0.0224 - val_mse: 0.1150\n",
            "Epoch 182/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0112 - mse: 0.1218 - val_loss: 0.0222 - val_mse: 0.1151\n",
            "Epoch 183/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0112 - mse: 0.1218 - val_loss: 0.0221 - val_mse: 0.1151\n",
            "Epoch 184/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0111 - mse: 0.1218 - val_loss: 0.0219 - val_mse: 0.1151\n",
            "Epoch 185/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0112 - mse: 0.1218 - val_loss: 0.0218 - val_mse: 0.1152\n",
            "Epoch 186/800\n",
            "2/2 [==============================] - 1s 259ms/step - loss: 0.0112 - mse: 0.1219 - val_loss: 0.0216 - val_mse: 0.1152\n",
            "Epoch 187/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0112 - mse: 0.1219 - val_loss: 0.0215 - val_mse: 0.1152\n",
            "Epoch 188/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0110 - mse: 0.1220 - val_loss: 0.0213 - val_mse: 0.1153\n",
            "Epoch 189/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0111 - mse: 0.1220 - val_loss: 0.0212 - val_mse: 0.1153\n",
            "Epoch 190/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0111 - mse: 0.1220 - val_loss: 0.0211 - val_mse: 0.1153\n",
            "Epoch 191/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0110 - mse: 0.1221 - val_loss: 0.0210 - val_mse: 0.1154\n",
            "Epoch 192/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0111 - mse: 0.1221 - val_loss: 0.0208 - val_mse: 0.1154\n",
            "Epoch 193/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0110 - mse: 0.1221 - val_loss: 0.0207 - val_mse: 0.1154\n",
            "Epoch 194/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0109 - mse: 0.1221 - val_loss: 0.0205 - val_mse: 0.1155\n",
            "Epoch 195/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0109 - mse: 0.1222 - val_loss: 0.0203 - val_mse: 0.1155\n",
            "Epoch 196/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0109 - mse: 0.1222 - val_loss: 0.0201 - val_mse: 0.1155\n",
            "Epoch 197/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0109 - mse: 0.1223 - val_loss: 0.0200 - val_mse: 0.1155\n",
            "Epoch 198/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0108 - mse: 0.1223 - val_loss: 0.0198 - val_mse: 0.1156\n",
            "Epoch 199/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0108 - mse: 0.1224 - val_loss: 0.0197 - val_mse: 0.1156\n",
            "Epoch 200/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0108 - mse: 0.1223 - val_loss: 0.0195 - val_mse: 0.1156\n",
            "Epoch 201/800\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.0108 - mse: 0.1224 - val_loss: 0.0194 - val_mse: 0.1157\n",
            "Epoch 202/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0108 - mse: 0.1224 - val_loss: 0.0192 - val_mse: 0.1157\n",
            "Epoch 203/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0108 - mse: 0.1225 - val_loss: 0.0191 - val_mse: 0.1158\n",
            "Epoch 204/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0109 - mse: 0.1225 - val_loss: 0.0190 - val_mse: 0.1158\n",
            "Epoch 205/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0108 - mse: 0.1225 - val_loss: 0.0189 - val_mse: 0.1158\n",
            "Epoch 206/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0107 - mse: 0.1226 - val_loss: 0.0188 - val_mse: 0.1159\n",
            "Epoch 207/800\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.0107 - mse: 0.1226 - val_loss: 0.0187 - val_mse: 0.1159\n",
            "Epoch 208/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0106 - mse: 0.1226 - val_loss: 0.0186 - val_mse: 0.1159\n",
            "Epoch 209/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0106 - mse: 0.1226 - val_loss: 0.0186 - val_mse: 0.1159\n",
            "Epoch 210/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0106 - mse: 0.1227 - val_loss: 0.0185 - val_mse: 0.1160\n",
            "Epoch 211/800\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.0105 - mse: 0.1227 - val_loss: 0.0183 - val_mse: 0.1160\n",
            "Epoch 212/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0106 - mse: 0.1228 - val_loss: 0.0182 - val_mse: 0.1160\n",
            "Epoch 213/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0106 - mse: 0.1228 - val_loss: 0.0181 - val_mse: 0.1161\n",
            "Epoch 214/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0105 - mse: 0.1229 - val_loss: 0.0180 - val_mse: 0.1161\n",
            "Epoch 215/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0105 - mse: 0.1230 - val_loss: 0.0178 - val_mse: 0.1161\n",
            "Epoch 216/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0105 - mse: 0.1229 - val_loss: 0.0177 - val_mse: 0.1161\n",
            "Epoch 217/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0106 - mse: 0.1231 - val_loss: 0.0176 - val_mse: 0.1162\n",
            "Epoch 218/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0105 - mse: 0.1231 - val_loss: 0.0175 - val_mse: 0.1162\n",
            "Epoch 219/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0105 - mse: 0.1231 - val_loss: 0.0174 - val_mse: 0.1162\n",
            "Epoch 220/800\n",
            "2/2 [==============================] - 1s 285ms/step - loss: 0.0104 - mse: 0.1232 - val_loss: 0.0173 - val_mse: 0.1162\n",
            "Epoch 221/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0104 - mse: 0.1232 - val_loss: 0.0172 - val_mse: 0.1163\n",
            "Epoch 222/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0103 - mse: 0.1232 - val_loss: 0.0171 - val_mse: 0.1163\n",
            "Epoch 223/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0104 - mse: 0.1232 - val_loss: 0.0171 - val_mse: 0.1163\n",
            "Epoch 224/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0104 - mse: 0.1233 - val_loss: 0.0169 - val_mse: 0.1163\n",
            "Epoch 225/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0103 - mse: 0.1233 - val_loss: 0.0168 - val_mse: 0.1163\n",
            "Epoch 226/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0103 - mse: 0.1233 - val_loss: 0.0167 - val_mse: 0.1164\n",
            "Epoch 227/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0103 - mse: 0.1234 - val_loss: 0.0165 - val_mse: 0.1164\n",
            "Epoch 228/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0103 - mse: 0.1235 - val_loss: 0.0164 - val_mse: 0.1164\n",
            "Epoch 229/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0103 - mse: 0.1235 - val_loss: 0.0162 - val_mse: 0.1165\n",
            "Epoch 230/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0103 - mse: 0.1235 - val_loss: 0.0161 - val_mse: 0.1165\n",
            "Epoch 231/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0102 - mse: 0.1236 - val_loss: 0.0160 - val_mse: 0.1166\n",
            "Epoch 232/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0102 - mse: 0.1236 - val_loss: 0.0159 - val_mse: 0.1166\n",
            "Epoch 233/800\n",
            "2/2 [==============================] - 1s 257ms/step - loss: 0.0102 - mse: 0.1237 - val_loss: 0.0158 - val_mse: 0.1167\n",
            "Epoch 234/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0101 - mse: 0.1237 - val_loss: 0.0157 - val_mse: 0.1167\n",
            "Epoch 235/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0102 - mse: 0.1238 - val_loss: 0.0156 - val_mse: 0.1167\n",
            "Epoch 236/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0100 - mse: 0.1238 - val_loss: 0.0155 - val_mse: 0.1168\n",
            "Epoch 237/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0101 - mse: 0.1239 - val_loss: 0.0154 - val_mse: 0.1168\n",
            "Epoch 238/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0100 - mse: 0.1239 - val_loss: 0.0153 - val_mse: 0.1169\n",
            "Epoch 239/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0101 - mse: 0.1240 - val_loss: 0.0152 - val_mse: 0.1169\n",
            "Epoch 240/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0102 - mse: 0.1240 - val_loss: 0.0151 - val_mse: 0.1170\n",
            "Epoch 241/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0100 - mse: 0.1241 - val_loss: 0.0150 - val_mse: 0.1170\n",
            "Epoch 242/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0101 - mse: 0.1241 - val_loss: 0.0149 - val_mse: 0.1171\n",
            "Epoch 243/800\n",
            "2/2 [==============================] - 1s 259ms/step - loss: 0.0100 - mse: 0.1242 - val_loss: 0.0149 - val_mse: 0.1171\n",
            "Epoch 244/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0101 - mse: 0.1242 - val_loss: 0.0148 - val_mse: 0.1171\n",
            "Epoch 245/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0100 - mse: 0.1242 - val_loss: 0.0147 - val_mse: 0.1172\n",
            "Epoch 246/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0099 - mse: 0.1242 - val_loss: 0.0146 - val_mse: 0.1172\n",
            "Epoch 247/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0100 - mse: 0.1243 - val_loss: 0.0146 - val_mse: 0.1172\n",
            "Epoch 248/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0099 - mse: 0.1243 - val_loss: 0.0145 - val_mse: 0.1172\n",
            "Epoch 249/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0099 - mse: 0.1243 - val_loss: 0.0144 - val_mse: 0.1172\n",
            "Epoch 250/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0100 - mse: 0.1244 - val_loss: 0.0143 - val_mse: 0.1173\n",
            "Epoch 251/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0099 - mse: 0.1244 - val_loss: 0.0143 - val_mse: 0.1173\n",
            "Epoch 252/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0099 - mse: 0.1245 - val_loss: 0.0142 - val_mse: 0.1174\n",
            "Epoch 253/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0099 - mse: 0.1245 - val_loss: 0.0141 - val_mse: 0.1174\n",
            "Epoch 254/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0098 - mse: 0.1246 - val_loss: 0.0140 - val_mse: 0.1174\n",
            "Epoch 255/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0098 - mse: 0.1247 - val_loss: 0.0139 - val_mse: 0.1175\n",
            "Epoch 256/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0098 - mse: 0.1247 - val_loss: 0.0139 - val_mse: 0.1176\n",
            "Epoch 257/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0098 - mse: 0.1248 - val_loss: 0.0138 - val_mse: 0.1177\n",
            "Epoch 258/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0098 - mse: 0.1248 - val_loss: 0.0137 - val_mse: 0.1177\n",
            "Epoch 259/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0097 - mse: 0.1248 - val_loss: 0.0137 - val_mse: 0.1178\n",
            "Epoch 260/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0097 - mse: 0.1248 - val_loss: 0.0136 - val_mse: 0.1178\n",
            "Epoch 261/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0097 - mse: 0.1248 - val_loss: 0.0136 - val_mse: 0.1178\n",
            "Epoch 262/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0098 - mse: 0.1250 - val_loss: 0.0135 - val_mse: 0.1179\n",
            "Epoch 263/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0097 - mse: 0.1249 - val_loss: 0.0135 - val_mse: 0.1179\n",
            "Epoch 264/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0097 - mse: 0.1250 - val_loss: 0.0134 - val_mse: 0.1180\n",
            "Epoch 265/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0097 - mse: 0.1251 - val_loss: 0.0133 - val_mse: 0.1180\n",
            "Epoch 266/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0097 - mse: 0.1252 - val_loss: 0.0132 - val_mse: 0.1180\n",
            "Epoch 267/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0096 - mse: 0.1252 - val_loss: 0.0132 - val_mse: 0.1181\n",
            "Epoch 268/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0096 - mse: 0.1253 - val_loss: 0.0131 - val_mse: 0.1182\n",
            "Epoch 269/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0097 - mse: 0.1252 - val_loss: 0.0130 - val_mse: 0.1182\n",
            "Epoch 270/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0095 - mse: 0.1253 - val_loss: 0.0129 - val_mse: 0.1182\n",
            "Epoch 271/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0095 - mse: 0.1254 - val_loss: 0.0129 - val_mse: 0.1183\n",
            "Epoch 272/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0095 - mse: 0.1254 - val_loss: 0.0128 - val_mse: 0.1183\n",
            "Epoch 273/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0096 - mse: 0.1254 - val_loss: 0.0128 - val_mse: 0.1183\n",
            "Epoch 274/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0095 - mse: 0.1255 - val_loss: 0.0127 - val_mse: 0.1184\n",
            "Epoch 275/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0096 - mse: 0.1256 - val_loss: 0.0126 - val_mse: 0.1184\n",
            "Epoch 276/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0096 - mse: 0.1256 - val_loss: 0.0126 - val_mse: 0.1185\n",
            "Epoch 277/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0095 - mse: 0.1257 - val_loss: 0.0125 - val_mse: 0.1186\n",
            "Epoch 278/800\n",
            "2/2 [==============================] - 1s 259ms/step - loss: 0.0095 - mse: 0.1258 - val_loss: 0.0124 - val_mse: 0.1187\n",
            "Epoch 279/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0094 - mse: 0.1258 - val_loss: 0.0123 - val_mse: 0.1187\n",
            "Epoch 280/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0095 - mse: 0.1260 - val_loss: 0.0123 - val_mse: 0.1188\n",
            "Epoch 281/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0094 - mse: 0.1260 - val_loss: 0.0122 - val_mse: 0.1189\n",
            "Epoch 282/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0094 - mse: 0.1259 - val_loss: 0.0121 - val_mse: 0.1190\n",
            "Epoch 283/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0094 - mse: 0.1260 - val_loss: 0.0121 - val_mse: 0.1191\n",
            "Epoch 284/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0094 - mse: 0.1261 - val_loss: 0.0120 - val_mse: 0.1192\n",
            "Epoch 285/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0094 - mse: 0.1262 - val_loss: 0.0120 - val_mse: 0.1193\n",
            "Epoch 286/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0094 - mse: 0.1262 - val_loss: 0.0119 - val_mse: 0.1193\n",
            "Epoch 287/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0094 - mse: 0.1263 - val_loss: 0.0119 - val_mse: 0.1194\n",
            "Epoch 288/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0092 - mse: 0.1264 - val_loss: 0.0119 - val_mse: 0.1194\n",
            "Epoch 289/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0093 - mse: 0.1264 - val_loss: 0.0118 - val_mse: 0.1195\n",
            "Epoch 290/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0092 - mse: 0.1264 - val_loss: 0.0118 - val_mse: 0.1195\n",
            "Epoch 291/800\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.0092 - mse: 0.1266 - val_loss: 0.0117 - val_mse: 0.1196\n",
            "Epoch 292/800\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.0092 - mse: 0.1266 - val_loss: 0.0117 - val_mse: 0.1196\n",
            "Epoch 293/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0093 - mse: 0.1267 - val_loss: 0.0116 - val_mse: 0.1197\n",
            "Epoch 294/800\n",
            "2/2 [==============================] - 1s 282ms/step - loss: 0.0092 - mse: 0.1268 - val_loss: 0.0116 - val_mse: 0.1198\n",
            "Epoch 295/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0093 - mse: 0.1269 - val_loss: 0.0116 - val_mse: 0.1199\n",
            "Epoch 296/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0092 - mse: 0.1269 - val_loss: 0.0115 - val_mse: 0.1199\n",
            "Epoch 297/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0092 - mse: 0.1269 - val_loss: 0.0115 - val_mse: 0.1200\n",
            "Epoch 298/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0093 - mse: 0.1271 - val_loss: 0.0115 - val_mse: 0.1201\n",
            "Epoch 299/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0091 - mse: 0.1270 - val_loss: 0.0114 - val_mse: 0.1201\n",
            "Epoch 300/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0091 - mse: 0.1271 - val_loss: 0.0114 - val_mse: 0.1202\n",
            "Epoch 301/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0092 - mse: 0.1271 - val_loss: 0.0113 - val_mse: 0.1203\n",
            "Epoch 302/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0091 - mse: 0.1272 - val_loss: 0.0113 - val_mse: 0.1204\n",
            "Epoch 303/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0092 - mse: 0.1273 - val_loss: 0.0113 - val_mse: 0.1205\n",
            "Epoch 304/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0091 - mse: 0.1273 - val_loss: 0.0112 - val_mse: 0.1206\n",
            "Epoch 305/800\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.0090 - mse: 0.1274 - val_loss: 0.0112 - val_mse: 0.1207\n",
            "Epoch 306/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0092 - mse: 0.1274 - val_loss: 0.0111 - val_mse: 0.1208\n",
            "Epoch 307/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0091 - mse: 0.1275 - val_loss: 0.0111 - val_mse: 0.1209\n",
            "Epoch 308/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0090 - mse: 0.1276 - val_loss: 0.0110 - val_mse: 0.1209\n",
            "Epoch 309/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0090 - mse: 0.1277 - val_loss: 0.0110 - val_mse: 0.1210\n",
            "Epoch 310/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0090 - mse: 0.1277 - val_loss: 0.0109 - val_mse: 0.1211\n",
            "Epoch 311/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0090 - mse: 0.1277 - val_loss: 0.0109 - val_mse: 0.1212\n",
            "Epoch 312/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0090 - mse: 0.1278 - val_loss: 0.0109 - val_mse: 0.1212\n",
            "Epoch 313/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0090 - mse: 0.1278 - val_loss: 0.0108 - val_mse: 0.1213\n",
            "Epoch 314/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0090 - mse: 0.1280 - val_loss: 0.0108 - val_mse: 0.1214\n",
            "Epoch 315/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0089 - mse: 0.1280 - val_loss: 0.0108 - val_mse: 0.1215\n",
            "Epoch 316/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0090 - mse: 0.1280 - val_loss: 0.0107 - val_mse: 0.1216\n",
            "Epoch 317/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0089 - mse: 0.1281 - val_loss: 0.0107 - val_mse: 0.1217\n",
            "Epoch 318/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0089 - mse: 0.1282 - val_loss: 0.0107 - val_mse: 0.1219\n",
            "Epoch 319/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0089 - mse: 0.1283 - val_loss: 0.0106 - val_mse: 0.1220\n",
            "Epoch 320/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0090 - mse: 0.1284 - val_loss: 0.0106 - val_mse: 0.1221\n",
            "Epoch 321/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0089 - mse: 0.1284 - val_loss: 0.0106 - val_mse: 0.1222\n",
            "Epoch 322/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0090 - mse: 0.1285 - val_loss: 0.0105 - val_mse: 0.1223\n",
            "Epoch 323/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0090 - mse: 0.1285 - val_loss: 0.0105 - val_mse: 0.1224\n",
            "Epoch 324/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0089 - mse: 0.1285 - val_loss: 0.0105 - val_mse: 0.1225\n",
            "Epoch 325/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0089 - mse: 0.1286 - val_loss: 0.0105 - val_mse: 0.1226\n",
            "Epoch 326/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0088 - mse: 0.1287 - val_loss: 0.0105 - val_mse: 0.1227\n",
            "Epoch 327/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0088 - mse: 0.1288 - val_loss: 0.0104 - val_mse: 0.1228\n",
            "Epoch 328/800\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.0089 - mse: 0.1289 - val_loss: 0.0104 - val_mse: 0.1229\n",
            "Epoch 329/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0088 - mse: 0.1289 - val_loss: 0.0104 - val_mse: 0.1229\n",
            "Epoch 330/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0087 - mse: 0.1289 - val_loss: 0.0104 - val_mse: 0.1230\n",
            "Epoch 331/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0088 - mse: 0.1290 - val_loss: 0.0104 - val_mse: 0.1231\n",
            "Epoch 332/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0087 - mse: 0.1291 - val_loss: 0.0103 - val_mse: 0.1232\n",
            "Epoch 333/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0087 - mse: 0.1291 - val_loss: 0.0103 - val_mse: 0.1232\n",
            "Epoch 334/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0088 - mse: 0.1291 - val_loss: 0.0102 - val_mse: 0.1233\n",
            "Epoch 335/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0087 - mse: 0.1292 - val_loss: 0.0102 - val_mse: 0.1234\n",
            "Epoch 336/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0087 - mse: 0.1293 - val_loss: 0.0102 - val_mse: 0.1234\n",
            "Epoch 337/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0087 - mse: 0.1293 - val_loss: 0.0102 - val_mse: 0.1235\n",
            "Epoch 338/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0088 - mse: 0.1294 - val_loss: 0.0101 - val_mse: 0.1235\n",
            "Epoch 339/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0087 - mse: 0.1294 - val_loss: 0.0101 - val_mse: 0.1236\n",
            "Epoch 340/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0086 - mse: 0.1295 - val_loss: 0.0101 - val_mse: 0.1237\n",
            "Epoch 341/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0086 - mse: 0.1295 - val_loss: 0.0100 - val_mse: 0.1239\n",
            "Epoch 342/800\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.0087 - mse: 0.1296 - val_loss: 0.0100 - val_mse: 0.1240\n",
            "Epoch 343/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0086 - mse: 0.1297 - val_loss: 0.0100 - val_mse: 0.1241\n",
            "Epoch 344/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0086 - mse: 0.1297 - val_loss: 0.0100 - val_mse: 0.1242\n",
            "Epoch 345/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0086 - mse: 0.1297 - val_loss: 0.0100 - val_mse: 0.1243\n",
            "Epoch 346/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0086 - mse: 0.1299 - val_loss: 0.0099 - val_mse: 0.1244\n",
            "Epoch 347/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0086 - mse: 0.1299 - val_loss: 0.0099 - val_mse: 0.1245\n",
            "Epoch 348/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0085 - mse: 0.1299 - val_loss: 0.0099 - val_mse: 0.1246\n",
            "Epoch 349/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0086 - mse: 0.1301 - val_loss: 0.0099 - val_mse: 0.1247\n",
            "Epoch 350/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0086 - mse: 0.1301 - val_loss: 0.0099 - val_mse: 0.1248\n",
            "Epoch 351/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0085 - mse: 0.1302 - val_loss: 0.0098 - val_mse: 0.1249\n",
            "Epoch 352/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0085 - mse: 0.1302 - val_loss: 0.0098 - val_mse: 0.1250\n",
            "Epoch 353/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0085 - mse: 0.1303 - val_loss: 0.0098 - val_mse: 0.1251\n",
            "Epoch 354/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0084 - mse: 0.1304 - val_loss: 0.0098 - val_mse: 0.1252\n",
            "Epoch 355/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0085 - mse: 0.1304 - val_loss: 0.0098 - val_mse: 0.1253\n",
            "Epoch 356/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0085 - mse: 0.1306 - val_loss: 0.0097 - val_mse: 0.1254\n",
            "Epoch 357/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0085 - mse: 0.1305 - val_loss: 0.0097 - val_mse: 0.1255\n",
            "Epoch 358/800\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.0084 - mse: 0.1305 - val_loss: 0.0097 - val_mse: 0.1256\n",
            "Epoch 359/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0085 - mse: 0.1307 - val_loss: 0.0097 - val_mse: 0.1257\n",
            "Epoch 360/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0084 - mse: 0.1307 - val_loss: 0.0097 - val_mse: 0.1257\n",
            "Epoch 361/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0084 - mse: 0.1306 - val_loss: 0.0097 - val_mse: 0.1258\n",
            "Epoch 362/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0085 - mse: 0.1308 - val_loss: 0.0097 - val_mse: 0.1259\n",
            "Epoch 363/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0084 - mse: 0.1307 - val_loss: 0.0096 - val_mse: 0.1260\n",
            "Epoch 364/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0084 - mse: 0.1308 - val_loss: 0.0096 - val_mse: 0.1260\n",
            "Epoch 365/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0084 - mse: 0.1309 - val_loss: 0.0096 - val_mse: 0.1261\n",
            "Epoch 366/800\n",
            "2/2 [==============================] - 1s 256ms/step - loss: 0.0084 - mse: 0.1310 - val_loss: 0.0096 - val_mse: 0.1262\n",
            "Epoch 367/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0083 - mse: 0.1311 - val_loss: 0.0096 - val_mse: 0.1263\n",
            "Epoch 368/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0084 - mse: 0.1311 - val_loss: 0.0095 - val_mse: 0.1264\n",
            "Epoch 369/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0083 - mse: 0.1313 - val_loss: 0.0095 - val_mse: 0.1266\n",
            "Epoch 370/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0084 - mse: 0.1314 - val_loss: 0.0095 - val_mse: 0.1266\n",
            "Epoch 371/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0083 - mse: 0.1314 - val_loss: 0.0095 - val_mse: 0.1267\n",
            "Epoch 372/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0083 - mse: 0.1315 - val_loss: 0.0095 - val_mse: 0.1268\n",
            "Epoch 373/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0083 - mse: 0.1315 - val_loss: 0.0095 - val_mse: 0.1269\n",
            "Epoch 374/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0083 - mse: 0.1316 - val_loss: 0.0094 - val_mse: 0.1270\n",
            "Epoch 375/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0083 - mse: 0.1317 - val_loss: 0.0094 - val_mse: 0.1271\n",
            "Epoch 376/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0082 - mse: 0.1316 - val_loss: 0.0094 - val_mse: 0.1273\n",
            "Epoch 377/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0083 - mse: 0.1318 - val_loss: 0.0094 - val_mse: 0.1274\n",
            "Epoch 378/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0083 - mse: 0.1318 - val_loss: 0.0094 - val_mse: 0.1275\n",
            "Epoch 379/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0083 - mse: 0.1319 - val_loss: 0.0094 - val_mse: 0.1276\n",
            "Epoch 380/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0083 - mse: 0.1319 - val_loss: 0.0094 - val_mse: 0.1277\n",
            "Epoch 381/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0082 - mse: 0.1321 - val_loss: 0.0094 - val_mse: 0.1279\n",
            "Epoch 382/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0082 - mse: 0.1321 - val_loss: 0.0094 - val_mse: 0.1280\n",
            "Epoch 383/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0082 - mse: 0.1321 - val_loss: 0.0094 - val_mse: 0.1281\n",
            "Epoch 384/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0082 - mse: 0.1322 - val_loss: 0.0094 - val_mse: 0.1282\n",
            "Epoch 385/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0082 - mse: 0.1323 - val_loss: 0.0093 - val_mse: 0.1283\n",
            "Epoch 386/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0082 - mse: 0.1323 - val_loss: 0.0093 - val_mse: 0.1284\n",
            "Epoch 387/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0082 - mse: 0.1324 - val_loss: 0.0093 - val_mse: 0.1285\n",
            "Epoch 388/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0081 - mse: 0.1324 - val_loss: 0.0093 - val_mse: 0.1285\n",
            "Epoch 389/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0081 - mse: 0.1325 - val_loss: 0.0093 - val_mse: 0.1285\n",
            "Epoch 390/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0081 - mse: 0.1326 - val_loss: 0.0093 - val_mse: 0.1285\n",
            "Epoch 391/800\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.0082 - mse: 0.1325 - val_loss: 0.0093 - val_mse: 0.1285\n",
            "Epoch 392/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0081 - mse: 0.1327 - val_loss: 0.0093 - val_mse: 0.1285\n",
            "Epoch 393/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0080 - mse: 0.1327 - val_loss: 0.0092 - val_mse: 0.1285\n",
            "Epoch 394/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0081 - mse: 0.1329 - val_loss: 0.0092 - val_mse: 0.1285\n",
            "Epoch 395/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0080 - mse: 0.1329 - val_loss: 0.0092 - val_mse: 0.1286\n",
            "Epoch 396/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0080 - mse: 0.1330 - val_loss: 0.0092 - val_mse: 0.1287\n",
            "Epoch 397/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0081 - mse: 0.1330 - val_loss: 0.0092 - val_mse: 0.1288\n",
            "Epoch 398/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0081 - mse: 0.1332 - val_loss: 0.0092 - val_mse: 0.1289\n",
            "Epoch 399/800\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.0081 - mse: 0.1332 - val_loss: 0.0092 - val_mse: 0.1291\n",
            "Epoch 400/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0081 - mse: 0.1332 - val_loss: 0.0092 - val_mse: 0.1292\n",
            "Epoch 401/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0081 - mse: 0.1334 - val_loss: 0.0092 - val_mse: 0.1293\n",
            "Epoch 402/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0081 - mse: 0.1334 - val_loss: 0.0091 - val_mse: 0.1295\n",
            "Epoch 403/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0080 - mse: 0.1335 - val_loss: 0.0091 - val_mse: 0.1296\n",
            "Epoch 404/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0080 - mse: 0.1335 - val_loss: 0.0091 - val_mse: 0.1297\n",
            "Epoch 405/800\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.0080 - mse: 0.1336 - val_loss: 0.0091 - val_mse: 0.1297\n",
            "Epoch 406/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0079 - mse: 0.1337 - val_loss: 0.0091 - val_mse: 0.1297\n",
            "Epoch 407/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0080 - mse: 0.1337 - val_loss: 0.0091 - val_mse: 0.1298\n",
            "Epoch 408/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0080 - mse: 0.1337 - val_loss: 0.0091 - val_mse: 0.1298\n",
            "Epoch 409/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0081 - mse: 0.1337 - val_loss: 0.0091 - val_mse: 0.1299\n",
            "Epoch 410/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0080 - mse: 0.1339 - val_loss: 0.0090 - val_mse: 0.1300\n",
            "Epoch 411/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0079 - mse: 0.1339 - val_loss: 0.0090 - val_mse: 0.1301\n",
            "Epoch 412/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0079 - mse: 0.1339 - val_loss: 0.0090 - val_mse: 0.1301\n",
            "Epoch 413/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0080 - mse: 0.1340 - val_loss: 0.0090 - val_mse: 0.1302\n",
            "Epoch 414/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0079 - mse: 0.1340 - val_loss: 0.0090 - val_mse: 0.1302\n",
            "Epoch 415/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0079 - mse: 0.1340 - val_loss: 0.0090 - val_mse: 0.1302\n",
            "Epoch 416/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0079 - mse: 0.1341 - val_loss: 0.0090 - val_mse: 0.1302\n",
            "Epoch 417/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0079 - mse: 0.1341 - val_loss: 0.0090 - val_mse: 0.1302\n",
            "Epoch 418/800\n",
            "2/2 [==============================] - 1s 259ms/step - loss: 0.0079 - mse: 0.1342 - val_loss: 0.0090 - val_mse: 0.1303\n",
            "Epoch 419/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0079 - mse: 0.1342 - val_loss: 0.0089 - val_mse: 0.1304\n",
            "Epoch 420/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0079 - mse: 0.1343 - val_loss: 0.0089 - val_mse: 0.1305\n",
            "Epoch 421/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0078 - mse: 0.1344 - val_loss: 0.0089 - val_mse: 0.1306\n",
            "Epoch 422/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0078 - mse: 0.1344 - val_loss: 0.0089 - val_mse: 0.1308\n",
            "Epoch 423/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0079 - mse: 0.1345 - val_loss: 0.0089 - val_mse: 0.1310\n",
            "Epoch 424/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0078 - mse: 0.1346 - val_loss: 0.0089 - val_mse: 0.1313\n",
            "Epoch 425/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0079 - mse: 0.1346 - val_loss: 0.0089 - val_mse: 0.1315\n",
            "Epoch 426/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0078 - mse: 0.1347 - val_loss: 0.0089 - val_mse: 0.1316\n",
            "Epoch 427/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0078 - mse: 0.1347 - val_loss: 0.0089 - val_mse: 0.1317\n",
            "Epoch 428/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0078 - mse: 0.1349 - val_loss: 0.0089 - val_mse: 0.1317\n",
            "Epoch 429/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0078 - mse: 0.1349 - val_loss: 0.0089 - val_mse: 0.1318\n",
            "Epoch 430/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0078 - mse: 0.1351 - val_loss: 0.0089 - val_mse: 0.1319\n",
            "Epoch 431/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0077 - mse: 0.1351 - val_loss: 0.0089 - val_mse: 0.1319\n",
            "Epoch 432/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0078 - mse: 0.1351 - val_loss: 0.0089 - val_mse: 0.1320\n",
            "Epoch 433/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0078 - mse: 0.1352 - val_loss: 0.0088 - val_mse: 0.1322\n",
            "Epoch 434/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0078 - mse: 0.1353 - val_loss: 0.0088 - val_mse: 0.1323\n",
            "Epoch 435/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0077 - mse: 0.1353 - val_loss: 0.0088 - val_mse: 0.1324\n",
            "Epoch 436/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0077 - mse: 0.1353 - val_loss: 0.0088 - val_mse: 0.1326\n",
            "Epoch 437/800\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.0077 - mse: 0.1355 - val_loss: 0.0088 - val_mse: 0.1327\n",
            "Epoch 438/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0077 - mse: 0.1354 - val_loss: 0.0088 - val_mse: 0.1328\n",
            "Epoch 439/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0077 - mse: 0.1355 - val_loss: 0.0088 - val_mse: 0.1329\n",
            "Epoch 440/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0077 - mse: 0.1356 - val_loss: 0.0088 - val_mse: 0.1331\n",
            "Epoch 441/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0078 - mse: 0.1356 - val_loss: 0.0088 - val_mse: 0.1332\n",
            "Epoch 442/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0077 - mse: 0.1358 - val_loss: 0.0088 - val_mse: 0.1333\n",
            "Epoch 443/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0076 - mse: 0.1357 - val_loss: 0.0088 - val_mse: 0.1334\n",
            "Epoch 444/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0077 - mse: 0.1359 - val_loss: 0.0088 - val_mse: 0.1334\n",
            "Epoch 445/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0077 - mse: 0.1358 - val_loss: 0.0087 - val_mse: 0.1334\n",
            "Epoch 446/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0077 - mse: 0.1359 - val_loss: 0.0087 - val_mse: 0.1335\n",
            "Epoch 447/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0076 - mse: 0.1359 - val_loss: 0.0087 - val_mse: 0.1337\n",
            "Epoch 448/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0076 - mse: 0.1360 - val_loss: 0.0087 - val_mse: 0.1338\n",
            "Epoch 449/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0078 - mse: 0.1360 - val_loss: 0.0087 - val_mse: 0.1340\n",
            "Epoch 450/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0076 - mse: 0.1362 - val_loss: 0.0087 - val_mse: 0.1341\n",
            "Epoch 451/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0077 - mse: 0.1362 - val_loss: 0.0087 - val_mse: 0.1342\n",
            "Epoch 452/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0077 - mse: 0.1363 - val_loss: 0.0087 - val_mse: 0.1342\n",
            "Epoch 453/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0076 - mse: 0.1365 - val_loss: 0.0087 - val_mse: 0.1342\n",
            "Epoch 454/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0076 - mse: 0.1364 - val_loss: 0.0087 - val_mse: 0.1343\n",
            "Epoch 455/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0076 - mse: 0.1365 - val_loss: 0.0087 - val_mse: 0.1343\n",
            "Epoch 456/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0076 - mse: 0.1365 - val_loss: 0.0087 - val_mse: 0.1344\n",
            "Epoch 457/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0076 - mse: 0.1365 - val_loss: 0.0086 - val_mse: 0.1345\n",
            "Epoch 458/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0076 - mse: 0.1366 - val_loss: 0.0086 - val_mse: 0.1346\n",
            "Epoch 459/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0075 - mse: 0.1366 - val_loss: 0.0086 - val_mse: 0.1347\n",
            "Epoch 460/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0076 - mse: 0.1367 - val_loss: 0.0086 - val_mse: 0.1350\n",
            "Epoch 461/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0076 - mse: 0.1370 - val_loss: 0.0086 - val_mse: 0.1352\n",
            "Epoch 462/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0076 - mse: 0.1369 - val_loss: 0.0086 - val_mse: 0.1353\n",
            "Epoch 463/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0075 - mse: 0.1370 - val_loss: 0.0086 - val_mse: 0.1355\n",
            "Epoch 464/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0076 - mse: 0.1372 - val_loss: 0.0086 - val_mse: 0.1356\n",
            "Epoch 465/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0075 - mse: 0.1373 - val_loss: 0.0086 - val_mse: 0.1357\n",
            "Epoch 466/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0076 - mse: 0.1373 - val_loss: 0.0086 - val_mse: 0.1359\n",
            "Epoch 467/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0075 - mse: 0.1373 - val_loss: 0.0086 - val_mse: 0.1360\n",
            "Epoch 468/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0075 - mse: 0.1373 - val_loss: 0.0086 - val_mse: 0.1362\n",
            "Epoch 469/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0075 - mse: 0.1374 - val_loss: 0.0086 - val_mse: 0.1364\n",
            "Epoch 470/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0075 - mse: 0.1375 - val_loss: 0.0086 - val_mse: 0.1366\n",
            "Epoch 471/800\n",
            "2/2 [==============================] - 1s 259ms/step - loss: 0.0076 - mse: 0.1374 - val_loss: 0.0086 - val_mse: 0.1369\n",
            "Epoch 472/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0074 - mse: 0.1376 - val_loss: 0.0086 - val_mse: 0.1370\n",
            "Epoch 473/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0075 - mse: 0.1375 - val_loss: 0.0085 - val_mse: 0.1371\n",
            "Epoch 474/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0075 - mse: 0.1376 - val_loss: 0.0085 - val_mse: 0.1372\n",
            "Epoch 475/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0074 - mse: 0.1375 - val_loss: 0.0085 - val_mse: 0.1373\n",
            "Epoch 476/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0074 - mse: 0.1378 - val_loss: 0.0085 - val_mse: 0.1374\n",
            "Epoch 477/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0074 - mse: 0.1378 - val_loss: 0.0085 - val_mse: 0.1375\n",
            "Epoch 478/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0075 - mse: 0.1379 - val_loss: 0.0085 - val_mse: 0.1375\n",
            "Epoch 479/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0075 - mse: 0.1379 - val_loss: 0.0085 - val_mse: 0.1375\n",
            "Epoch 480/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0075 - mse: 0.1380 - val_loss: 0.0085 - val_mse: 0.1374\n",
            "Epoch 481/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0075 - mse: 0.1381 - val_loss: 0.0085 - val_mse: 0.1373\n",
            "Epoch 482/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0075 - mse: 0.1381 - val_loss: 0.0085 - val_mse: 0.1372\n",
            "Epoch 483/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0075 - mse: 0.1381 - val_loss: 0.0085 - val_mse: 0.1371\n",
            "Epoch 484/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0074 - mse: 0.1382 - val_loss: 0.0085 - val_mse: 0.1369\n",
            "Epoch 485/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0074 - mse: 0.1382 - val_loss: 0.0085 - val_mse: 0.1368\n",
            "Epoch 486/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0074 - mse: 0.1383 - val_loss: 0.0085 - val_mse: 0.1368\n",
            "Epoch 487/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0074 - mse: 0.1383 - val_loss: 0.0085 - val_mse: 0.1368\n",
            "Epoch 488/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0074 - mse: 0.1384 - val_loss: 0.0085 - val_mse: 0.1368\n",
            "Epoch 489/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0074 - mse: 0.1384 - val_loss: 0.0085 - val_mse: 0.1369\n",
            "Epoch 490/800\n",
            "2/2 [==============================] - 1s 284ms/step - loss: 0.0073 - mse: 0.1384 - val_loss: 0.0084 - val_mse: 0.1369\n",
            "Epoch 491/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0074 - mse: 0.1387 - val_loss: 0.0084 - val_mse: 0.1370\n",
            "Epoch 492/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0075 - mse: 0.1387 - val_loss: 0.0084 - val_mse: 0.1371\n",
            "Epoch 493/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0073 - mse: 0.1388 - val_loss: 0.0084 - val_mse: 0.1371\n",
            "Epoch 494/800\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.0074 - mse: 0.1389 - val_loss: 0.0084 - val_mse: 0.1372\n",
            "Epoch 495/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0074 - mse: 0.1390 - val_loss: 0.0084 - val_mse: 0.1372\n",
            "Epoch 496/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0074 - mse: 0.1390 - val_loss: 0.0084 - val_mse: 0.1373\n",
            "Epoch 497/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0074 - mse: 0.1390 - val_loss: 0.0084 - val_mse: 0.1373\n",
            "Epoch 498/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0073 - mse: 0.1390 - val_loss: 0.0084 - val_mse: 0.1373\n",
            "Epoch 499/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0073 - mse: 0.1391 - val_loss: 0.0084 - val_mse: 0.1374\n",
            "Epoch 500/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0073 - mse: 0.1393 - val_loss: 0.0084 - val_mse: 0.1374\n",
            "Epoch 501/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0073 - mse: 0.1393 - val_loss: 0.0084 - val_mse: 0.1374\n",
            "Epoch 502/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0073 - mse: 0.1394 - val_loss: 0.0083 - val_mse: 0.1373\n",
            "Epoch 503/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0073 - mse: 0.1394 - val_loss: 0.0083 - val_mse: 0.1373\n",
            "Epoch 504/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0073 - mse: 0.1394 - val_loss: 0.0083 - val_mse: 0.1373\n",
            "Epoch 505/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0072 - mse: 0.1395 - val_loss: 0.0083 - val_mse: 0.1372\n",
            "Epoch 506/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0072 - mse: 0.1395 - val_loss: 0.0083 - val_mse: 0.1372\n",
            "Epoch 507/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0073 - mse: 0.1395 - val_loss: 0.0083 - val_mse: 0.1373\n",
            "Epoch 508/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0073 - mse: 0.1397 - val_loss: 0.0083 - val_mse: 0.1375\n",
            "Epoch 509/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0073 - mse: 0.1397 - val_loss: 0.0083 - val_mse: 0.1379\n",
            "Epoch 510/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0073 - mse: 0.1398 - val_loss: 0.0083 - val_mse: 0.1382\n",
            "Epoch 511/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0072 - mse: 0.1397 - val_loss: 0.0083 - val_mse: 0.1384\n",
            "Epoch 512/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0072 - mse: 0.1398 - val_loss: 0.0083 - val_mse: 0.1386\n",
            "Epoch 513/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0073 - mse: 0.1398 - val_loss: 0.0083 - val_mse: 0.1389\n",
            "Epoch 514/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0072 - mse: 0.1399 - val_loss: 0.0083 - val_mse: 0.1391\n",
            "Epoch 515/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0072 - mse: 0.1399 - val_loss: 0.0083 - val_mse: 0.1392\n",
            "Epoch 516/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0072 - mse: 0.1400 - val_loss: 0.0083 - val_mse: 0.1393\n",
            "Epoch 517/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0072 - mse: 0.1400 - val_loss: 0.0083 - val_mse: 0.1393\n",
            "Epoch 518/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0072 - mse: 0.1401 - val_loss: 0.0083 - val_mse: 0.1393\n",
            "Epoch 519/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0072 - mse: 0.1402 - val_loss: 0.0083 - val_mse: 0.1392\n",
            "Epoch 520/800\n",
            "2/2 [==============================] - 1s 283ms/step - loss: 0.0072 - mse: 0.1402 - val_loss: 0.0083 - val_mse: 0.1391\n",
            "Epoch 521/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0072 - mse: 0.1401 - val_loss: 0.0083 - val_mse: 0.1391\n",
            "Epoch 522/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0072 - mse: 0.1402 - val_loss: 0.0083 - val_mse: 0.1393\n",
            "Epoch 523/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0072 - mse: 0.1404 - val_loss: 0.0083 - val_mse: 0.1394\n",
            "Epoch 524/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0071 - mse: 0.1403 - val_loss: 0.0082 - val_mse: 0.1398\n",
            "Epoch 525/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0071 - mse: 0.1404 - val_loss: 0.0082 - val_mse: 0.1400\n",
            "Epoch 526/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0072 - mse: 0.1406 - val_loss: 0.0082 - val_mse: 0.1401\n",
            "Epoch 527/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0072 - mse: 0.1406 - val_loss: 0.0082 - val_mse: 0.1401\n",
            "Epoch 528/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0072 - mse: 0.1408 - val_loss: 0.0083 - val_mse: 0.1402\n",
            "Epoch 529/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0072 - mse: 0.1407 - val_loss: 0.0083 - val_mse: 0.1402\n",
            "Epoch 530/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0071 - mse: 0.1407 - val_loss: 0.0082 - val_mse: 0.1403\n",
            "Epoch 531/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0071 - mse: 0.1411 - val_loss: 0.0082 - val_mse: 0.1404\n",
            "Epoch 532/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0071 - mse: 0.1410 - val_loss: 0.0082 - val_mse: 0.1404\n",
            "Epoch 533/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0072 - mse: 0.1412 - val_loss: 0.0082 - val_mse: 0.1405\n",
            "Epoch 534/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0071 - mse: 0.1413 - val_loss: 0.0081 - val_mse: 0.1406\n",
            "Epoch 535/800\n",
            "2/2 [==============================] - 1s 283ms/step - loss: 0.0071 - mse: 0.1413 - val_loss: 0.0081 - val_mse: 0.1405\n",
            "Epoch 536/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0071 - mse: 0.1414 - val_loss: 0.0081 - val_mse: 0.1405\n",
            "Epoch 537/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0072 - mse: 0.1415 - val_loss: 0.0081 - val_mse: 0.1405\n",
            "Epoch 538/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0071 - mse: 0.1414 - val_loss: 0.0081 - val_mse: 0.1405\n",
            "Epoch 539/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0071 - mse: 0.1414 - val_loss: 0.0082 - val_mse: 0.1406\n",
            "Epoch 540/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0071 - mse: 0.1414 - val_loss: 0.0082 - val_mse: 0.1407\n",
            "Epoch 541/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0071 - mse: 0.1416 - val_loss: 0.0082 - val_mse: 0.1408\n",
            "Epoch 542/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0072 - mse: 0.1417 - val_loss: 0.0082 - val_mse: 0.1410\n",
            "Epoch 543/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0071 - mse: 0.1415 - val_loss: 0.0082 - val_mse: 0.1412\n",
            "Epoch 544/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0071 - mse: 0.1417 - val_loss: 0.0082 - val_mse: 0.1415\n",
            "Epoch 545/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0071 - mse: 0.1418 - val_loss: 0.0081 - val_mse: 0.1418\n",
            "Epoch 546/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0070 - mse: 0.1419 - val_loss: 0.0081 - val_mse: 0.1420\n",
            "Epoch 547/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0071 - mse: 0.1418 - val_loss: 0.0081 - val_mse: 0.1421\n",
            "Epoch 548/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0071 - mse: 0.1419 - val_loss: 0.0081 - val_mse: 0.1422\n",
            "Epoch 549/800\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.0071 - mse: 0.1420 - val_loss: 0.0081 - val_mse: 0.1422\n",
            "Epoch 550/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0070 - mse: 0.1420 - val_loss: 0.0081 - val_mse: 0.1422\n",
            "Epoch 551/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0070 - mse: 0.1421 - val_loss: 0.0081 - val_mse: 0.1421\n",
            "Epoch 552/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0070 - mse: 0.1421 - val_loss: 0.0081 - val_mse: 0.1420\n",
            "Epoch 553/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0070 - mse: 0.1421 - val_loss: 0.0081 - val_mse: 0.1420\n",
            "Epoch 554/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0070 - mse: 0.1421 - val_loss: 0.0081 - val_mse: 0.1420\n",
            "Epoch 555/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0070 - mse: 0.1421 - val_loss: 0.0081 - val_mse: 0.1420\n",
            "Epoch 556/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0070 - mse: 0.1421 - val_loss: 0.0081 - val_mse: 0.1421\n",
            "Epoch 557/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0071 - mse: 0.1423 - val_loss: 0.0081 - val_mse: 0.1422\n",
            "Epoch 558/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0071 - mse: 0.1423 - val_loss: 0.0081 - val_mse: 0.1425\n",
            "Epoch 559/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0070 - mse: 0.1424 - val_loss: 0.0080 - val_mse: 0.1427\n",
            "Epoch 560/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0071 - mse: 0.1425 - val_loss: 0.0080 - val_mse: 0.1428\n",
            "Epoch 561/800\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.0070 - mse: 0.1426 - val_loss: 0.0080 - val_mse: 0.1430\n",
            "Epoch 562/800\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.0070 - mse: 0.1427 - val_loss: 0.0080 - val_mse: 0.1431\n",
            "Epoch 563/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0071 - mse: 0.1428 - val_loss: 0.0080 - val_mse: 0.1433\n",
            "Epoch 564/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0070 - mse: 0.1429 - val_loss: 0.0081 - val_mse: 0.1433\n",
            "Epoch 565/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0070 - mse: 0.1429 - val_loss: 0.0081 - val_mse: 0.1432\n",
            "Epoch 566/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0070 - mse: 0.1430 - val_loss: 0.0081 - val_mse: 0.1431\n",
            "Epoch 567/800\n",
            "2/2 [==============================] - 1s 257ms/step - loss: 0.0070 - mse: 0.1432 - val_loss: 0.0081 - val_mse: 0.1430\n",
            "Epoch 568/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0069 - mse: 0.1432 - val_loss: 0.0081 - val_mse: 0.1430\n",
            "Epoch 569/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0070 - mse: 0.1433 - val_loss: 0.0081 - val_mse: 0.1430\n",
            "Epoch 570/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0070 - mse: 0.1435 - val_loss: 0.0080 - val_mse: 0.1429\n",
            "Epoch 571/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0070 - mse: 0.1434 - val_loss: 0.0080 - val_mse: 0.1428\n",
            "Epoch 572/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0068 - mse: 0.1435 - val_loss: 0.0080 - val_mse: 0.1428\n",
            "Epoch 573/800\n",
            "2/2 [==============================] - 1s 256ms/step - loss: 0.0069 - mse: 0.1436 - val_loss: 0.0080 - val_mse: 0.1428\n",
            "Epoch 574/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0070 - mse: 0.1436 - val_loss: 0.0081 - val_mse: 0.1429\n",
            "Epoch 575/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0069 - mse: 0.1436 - val_loss: 0.0081 - val_mse: 0.1429\n",
            "Epoch 576/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0069 - mse: 0.1435 - val_loss: 0.0081 - val_mse: 0.1430\n",
            "Epoch 577/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0069 - mse: 0.1436 - val_loss: 0.0081 - val_mse: 0.1430\n",
            "Epoch 578/800\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.0069 - mse: 0.1438 - val_loss: 0.0081 - val_mse: 0.1430\n",
            "Epoch 579/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0069 - mse: 0.1438 - val_loss: 0.0080 - val_mse: 0.1430\n",
            "Epoch 580/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0069 - mse: 0.1439 - val_loss: 0.0080 - val_mse: 0.1431\n",
            "Epoch 581/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0070 - mse: 0.1440 - val_loss: 0.0080 - val_mse: 0.1433\n",
            "Epoch 582/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0069 - mse: 0.1441 - val_loss: 0.0080 - val_mse: 0.1435\n",
            "Epoch 583/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0069 - mse: 0.1442 - val_loss: 0.0080 - val_mse: 0.1436\n",
            "Epoch 584/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0069 - mse: 0.1443 - val_loss: 0.0080 - val_mse: 0.1436\n",
            "Epoch 585/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0069 - mse: 0.1443 - val_loss: 0.0080 - val_mse: 0.1436\n",
            "Epoch 586/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0069 - mse: 0.1443 - val_loss: 0.0080 - val_mse: 0.1436\n",
            "Epoch 587/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0069 - mse: 0.1445 - val_loss: 0.0080 - val_mse: 0.1438\n",
            "Epoch 588/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0068 - mse: 0.1446 - val_loss: 0.0080 - val_mse: 0.1439\n",
            "Epoch 589/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0069 - mse: 0.1446 - val_loss: 0.0080 - val_mse: 0.1441\n",
            "Epoch 590/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0069 - mse: 0.1448 - val_loss: 0.0080 - val_mse: 0.1444\n",
            "Epoch 591/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0068 - mse: 0.1448 - val_loss: 0.0079 - val_mse: 0.1446\n",
            "Epoch 592/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0069 - mse: 0.1448 - val_loss: 0.0079 - val_mse: 0.1447\n",
            "Epoch 593/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0069 - mse: 0.1447 - val_loss: 0.0079 - val_mse: 0.1448\n",
            "Epoch 594/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0068 - mse: 0.1448 - val_loss: 0.0079 - val_mse: 0.1449\n",
            "Epoch 595/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0069 - mse: 0.1448 - val_loss: 0.0079 - val_mse: 0.1449\n",
            "Epoch 596/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0069 - mse: 0.1448 - val_loss: 0.0079 - val_mse: 0.1448\n",
            "Epoch 597/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0068 - mse: 0.1450 - val_loss: 0.0079 - val_mse: 0.1446\n",
            "Epoch 598/800\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.0069 - mse: 0.1449 - val_loss: 0.0079 - val_mse: 0.1444\n",
            "Epoch 599/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0069 - mse: 0.1449 - val_loss: 0.0079 - val_mse: 0.1443\n",
            "Epoch 600/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0069 - mse: 0.1450 - val_loss: 0.0079 - val_mse: 0.1441\n",
            "Epoch 601/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0069 - mse: 0.1451 - val_loss: 0.0079 - val_mse: 0.1440\n",
            "Epoch 602/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0069 - mse: 0.1452 - val_loss: 0.0079 - val_mse: 0.1441\n",
            "Epoch 603/800\n",
            "2/2 [==============================] - 1s 257ms/step - loss: 0.0068 - mse: 0.1451 - val_loss: 0.0079 - val_mse: 0.1444\n",
            "Epoch 604/800\n",
            "2/2 [==============================] - 1s 291ms/step - loss: 0.0068 - mse: 0.1452 - val_loss: 0.0078 - val_mse: 0.1446\n",
            "Epoch 605/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0069 - mse: 0.1453 - val_loss: 0.0079 - val_mse: 0.1448\n",
            "Epoch 606/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0068 - mse: 0.1456 - val_loss: 0.0079 - val_mse: 0.1448\n",
            "Epoch 607/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0069 - mse: 0.1454 - val_loss: 0.0079 - val_mse: 0.1450\n",
            "Epoch 608/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0069 - mse: 0.1456 - val_loss: 0.0079 - val_mse: 0.1451\n",
            "Epoch 609/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0068 - mse: 0.1457 - val_loss: 0.0079 - val_mse: 0.1453\n",
            "Epoch 610/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0068 - mse: 0.1457 - val_loss: 0.0079 - val_mse: 0.1454\n",
            "Epoch 611/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0069 - mse: 0.1458 - val_loss: 0.0078 - val_mse: 0.1455\n",
            "Epoch 612/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0068 - mse: 0.1458 - val_loss: 0.0078 - val_mse: 0.1456\n",
            "Epoch 613/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0068 - mse: 0.1458 - val_loss: 0.0078 - val_mse: 0.1456\n",
            "Epoch 614/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0068 - mse: 0.1459 - val_loss: 0.0078 - val_mse: 0.1454\n",
            "Epoch 615/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0068 - mse: 0.1459 - val_loss: 0.0078 - val_mse: 0.1452\n",
            "Epoch 616/800\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.0068 - mse: 0.1462 - val_loss: 0.0078 - val_mse: 0.1450\n",
            "Epoch 617/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0068 - mse: 0.1461 - val_loss: 0.0078 - val_mse: 0.1447\n",
            "Epoch 618/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0067 - mse: 0.1461 - val_loss: 0.0078 - val_mse: 0.1447\n",
            "Epoch 619/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0067 - mse: 0.1461 - val_loss: 0.0078 - val_mse: 0.1447\n",
            "Epoch 620/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0068 - mse: 0.1462 - val_loss: 0.0078 - val_mse: 0.1448\n",
            "Epoch 621/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0068 - mse: 0.1462 - val_loss: 0.0078 - val_mse: 0.1450\n",
            "Epoch 622/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0068 - mse: 0.1463 - val_loss: 0.0078 - val_mse: 0.1452\n",
            "Epoch 623/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0068 - mse: 0.1463 - val_loss: 0.0078 - val_mse: 0.1455\n",
            "Epoch 624/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0068 - mse: 0.1464 - val_loss: 0.0078 - val_mse: 0.1457\n",
            "Epoch 625/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0067 - mse: 0.1464 - val_loss: 0.0079 - val_mse: 0.1458\n",
            "Epoch 626/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0068 - mse: 0.1465 - val_loss: 0.0079 - val_mse: 0.1459\n",
            "Epoch 627/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0068 - mse: 0.1466 - val_loss: 0.0078 - val_mse: 0.1461\n",
            "Epoch 628/800\n",
            "2/2 [==============================] - 1s 257ms/step - loss: 0.0068 - mse: 0.1465 - val_loss: 0.0078 - val_mse: 0.1461\n",
            "Epoch 629/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0068 - mse: 0.1468 - val_loss: 0.0078 - val_mse: 0.1461\n",
            "Epoch 630/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0068 - mse: 0.1468 - val_loss: 0.0078 - val_mse: 0.1461\n",
            "Epoch 631/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0068 - mse: 0.1469 - val_loss: 0.0078 - val_mse: 0.1461\n",
            "Epoch 632/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0067 - mse: 0.1471 - val_loss: 0.0078 - val_mse: 0.1460\n",
            "Epoch 633/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0067 - mse: 0.1470 - val_loss: 0.0078 - val_mse: 0.1460\n",
            "Epoch 634/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0067 - mse: 0.1470 - val_loss: 0.0078 - val_mse: 0.1460\n",
            "Epoch 635/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0067 - mse: 0.1470 - val_loss: 0.0078 - val_mse: 0.1461\n",
            "Epoch 636/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0067 - mse: 0.1471 - val_loss: 0.0078 - val_mse: 0.1462\n",
            "Epoch 637/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0067 - mse: 0.1472 - val_loss: 0.0078 - val_mse: 0.1463\n",
            "Epoch 638/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0067 - mse: 0.1472 - val_loss: 0.0078 - val_mse: 0.1464\n",
            "Epoch 639/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0067 - mse: 0.1472 - val_loss: 0.0078 - val_mse: 0.1465\n",
            "Epoch 640/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0067 - mse: 0.1474 - val_loss: 0.0078 - val_mse: 0.1468\n",
            "Epoch 641/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0067 - mse: 0.1474 - val_loss: 0.0078 - val_mse: 0.1470\n",
            "Epoch 642/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0067 - mse: 0.1475 - val_loss: 0.0078 - val_mse: 0.1471\n",
            "Epoch 643/800\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.0067 - mse: 0.1477 - val_loss: 0.0078 - val_mse: 0.1472\n",
            "Epoch 644/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0067 - mse: 0.1478 - val_loss: 0.0079 - val_mse: 0.1473\n",
            "Epoch 645/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0067 - mse: 0.1479 - val_loss: 0.0079 - val_mse: 0.1474\n",
            "Epoch 646/800\n",
            "2/2 [==============================] - 1s 283ms/step - loss: 0.0066 - mse: 0.1479 - val_loss: 0.0079 - val_mse: 0.1474\n",
            "Epoch 647/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0067 - mse: 0.1480 - val_loss: 0.0079 - val_mse: 0.1473\n",
            "Epoch 648/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0067 - mse: 0.1482 - val_loss: 0.0079 - val_mse: 0.1472\n",
            "Epoch 649/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0067 - mse: 0.1480 - val_loss: 0.0079 - val_mse: 0.1472\n",
            "Epoch 650/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0067 - mse: 0.1481 - val_loss: 0.0079 - val_mse: 0.1471\n",
            "Epoch 651/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0067 - mse: 0.1482 - val_loss: 0.0079 - val_mse: 0.1469\n",
            "Epoch 652/800\n",
            "2/2 [==============================] - 1s 282ms/step - loss: 0.0067 - mse: 0.1481 - val_loss: 0.0078 - val_mse: 0.1467\n",
            "Epoch 653/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0067 - mse: 0.1481 - val_loss: 0.0078 - val_mse: 0.1465\n",
            "Epoch 654/800\n",
            "2/2 [==============================] - 1s 285ms/step - loss: 0.0068 - mse: 0.1482 - val_loss: 0.0078 - val_mse: 0.1464\n",
            "Epoch 655/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0067 - mse: 0.1483 - val_loss: 0.0078 - val_mse: 0.1464\n",
            "Epoch 656/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0067 - mse: 0.1484 - val_loss: 0.0078 - val_mse: 0.1464\n",
            "Epoch 657/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0067 - mse: 0.1485 - val_loss: 0.0078 - val_mse: 0.1464\n",
            "Epoch 658/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0066 - mse: 0.1484 - val_loss: 0.0079 - val_mse: 0.1463\n",
            "Epoch 659/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0066 - mse: 0.1485 - val_loss: 0.0079 - val_mse: 0.1463\n",
            "Epoch 660/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0066 - mse: 0.1486 - val_loss: 0.0079 - val_mse: 0.1463\n",
            "Epoch 661/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0067 - mse: 0.1487 - val_loss: 0.0079 - val_mse: 0.1466\n",
            "Epoch 662/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0066 - mse: 0.1488 - val_loss: 0.0078 - val_mse: 0.1470\n",
            "Epoch 663/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0066 - mse: 0.1490 - val_loss: 0.0078 - val_mse: 0.1473\n",
            "Epoch 664/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0066 - mse: 0.1489 - val_loss: 0.0077 - val_mse: 0.1475\n",
            "Epoch 665/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0066 - mse: 0.1490 - val_loss: 0.0077 - val_mse: 0.1475\n",
            "Epoch 666/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0066 - mse: 0.1492 - val_loss: 0.0077 - val_mse: 0.1475\n",
            "Epoch 667/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0067 - mse: 0.1492 - val_loss: 0.0078 - val_mse: 0.1475\n",
            "Epoch 668/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0066 - mse: 0.1492 - val_loss: 0.0078 - val_mse: 0.1474\n",
            "Epoch 669/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0067 - mse: 0.1491 - val_loss: 0.0078 - val_mse: 0.1473\n",
            "Epoch 670/800\n",
            "2/2 [==============================] - 1s 283ms/step - loss: 0.0067 - mse: 0.1493 - val_loss: 0.0078 - val_mse: 0.1474\n",
            "Epoch 671/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0066 - mse: 0.1492 - val_loss: 0.0078 - val_mse: 0.1476\n",
            "Epoch 672/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0066 - mse: 0.1492 - val_loss: 0.0078 - val_mse: 0.1478\n",
            "Epoch 673/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0066 - mse: 0.1494 - val_loss: 0.0077 - val_mse: 0.1479\n",
            "Epoch 674/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0066 - mse: 0.1494 - val_loss: 0.0077 - val_mse: 0.1481\n",
            "Epoch 675/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0066 - mse: 0.1494 - val_loss: 0.0077 - val_mse: 0.1484\n",
            "Epoch 676/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0066 - mse: 0.1496 - val_loss: 0.0077 - val_mse: 0.1487\n",
            "Epoch 677/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0066 - mse: 0.1497 - val_loss: 0.0077 - val_mse: 0.1489\n",
            "Epoch 678/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0066 - mse: 0.1496 - val_loss: 0.0077 - val_mse: 0.1489\n",
            "Epoch 679/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0066 - mse: 0.1496 - val_loss: 0.0078 - val_mse: 0.1490\n",
            "Epoch 680/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0066 - mse: 0.1497 - val_loss: 0.0078 - val_mse: 0.1491\n",
            "Epoch 681/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0066 - mse: 0.1498 - val_loss: 0.0078 - val_mse: 0.1491\n",
            "Epoch 682/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0065 - mse: 0.1499 - val_loss: 0.0078 - val_mse: 0.1490\n",
            "Epoch 683/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0066 - mse: 0.1500 - val_loss: 0.0078 - val_mse: 0.1489\n",
            "Epoch 684/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0066 - mse: 0.1501 - val_loss: 0.0077 - val_mse: 0.1488\n",
            "Epoch 685/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0065 - mse: 0.1499 - val_loss: 0.0077 - val_mse: 0.1487\n",
            "Epoch 686/800\n",
            "2/2 [==============================] - 1s 261ms/step - loss: 0.0066 - mse: 0.1500 - val_loss: 0.0077 - val_mse: 0.1487\n",
            "Epoch 687/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0065 - mse: 0.1500 - val_loss: 0.0077 - val_mse: 0.1485\n",
            "Epoch 688/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0066 - mse: 0.1500 - val_loss: 0.0077 - val_mse: 0.1483\n",
            "Epoch 689/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0065 - mse: 0.1501 - val_loss: 0.0077 - val_mse: 0.1481\n",
            "Epoch 690/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0066 - mse: 0.1501 - val_loss: 0.0078 - val_mse: 0.1480\n",
            "Epoch 691/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0065 - mse: 0.1502 - val_loss: 0.0077 - val_mse: 0.1481\n",
            "Epoch 692/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0066 - mse: 0.1503 - val_loss: 0.0077 - val_mse: 0.1481\n",
            "Epoch 693/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0066 - mse: 0.1503 - val_loss: 0.0078 - val_mse: 0.1482\n",
            "Epoch 694/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0065 - mse: 0.1506 - val_loss: 0.0077 - val_mse: 0.1483\n",
            "Epoch 695/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0065 - mse: 0.1504 - val_loss: 0.0078 - val_mse: 0.1484\n",
            "Epoch 696/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0066 - mse: 0.1506 - val_loss: 0.0078 - val_mse: 0.1486\n",
            "Epoch 697/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0065 - mse: 0.1505 - val_loss: 0.0078 - val_mse: 0.1488\n",
            "Epoch 698/800\n",
            "2/2 [==============================] - 1s 280ms/step - loss: 0.0066 - mse: 0.1506 - val_loss: 0.0078 - val_mse: 0.1489\n",
            "Epoch 699/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0066 - mse: 0.1507 - val_loss: 0.0079 - val_mse: 0.1489\n",
            "Epoch 700/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0065 - mse: 0.1509 - val_loss: 0.0078 - val_mse: 0.1487\n",
            "Epoch 701/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0066 - mse: 0.1508 - val_loss: 0.0078 - val_mse: 0.1485\n",
            "Epoch 702/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0065 - mse: 0.1509 - val_loss: 0.0077 - val_mse: 0.1483\n",
            "Epoch 703/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0065 - mse: 0.1510 - val_loss: 0.0077 - val_mse: 0.1482\n",
            "Epoch 704/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0065 - mse: 0.1509 - val_loss: 0.0077 - val_mse: 0.1483\n",
            "Epoch 705/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0065 - mse: 0.1509 - val_loss: 0.0077 - val_mse: 0.1484\n",
            "Epoch 706/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0065 - mse: 0.1510 - val_loss: 0.0077 - val_mse: 0.1486\n",
            "Epoch 707/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0065 - mse: 0.1510 - val_loss: 0.0077 - val_mse: 0.1487\n",
            "Epoch 708/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0065 - mse: 0.1511 - val_loss: 0.0077 - val_mse: 0.1487\n",
            "Epoch 709/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0065 - mse: 0.1511 - val_loss: 0.0077 - val_mse: 0.1489\n",
            "Epoch 710/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0065 - mse: 0.1514 - val_loss: 0.0077 - val_mse: 0.1492\n",
            "Epoch 711/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0065 - mse: 0.1513 - val_loss: 0.0077 - val_mse: 0.1493\n",
            "Epoch 712/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0065 - mse: 0.1515 - val_loss: 0.0077 - val_mse: 0.1495\n",
            "Epoch 713/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0065 - mse: 0.1517 - val_loss: 0.0077 - val_mse: 0.1497\n",
            "Epoch 714/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0065 - mse: 0.1516 - val_loss: 0.0077 - val_mse: 0.1498\n",
            "Epoch 715/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0065 - mse: 0.1517 - val_loss: 0.0077 - val_mse: 0.1499\n",
            "Epoch 716/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0065 - mse: 0.1517 - val_loss: 0.0077 - val_mse: 0.1500\n",
            "Epoch 717/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0065 - mse: 0.1518 - val_loss: 0.0077 - val_mse: 0.1500\n",
            "Epoch 718/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0065 - mse: 0.1517 - val_loss: 0.0076 - val_mse: 0.1499\n",
            "Epoch 719/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0065 - mse: 0.1518 - val_loss: 0.0076 - val_mse: 0.1497\n",
            "Epoch 720/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0065 - mse: 0.1518 - val_loss: 0.0076 - val_mse: 0.1495\n",
            "Epoch 721/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0065 - mse: 0.1518 - val_loss: 0.0077 - val_mse: 0.1493\n",
            "Epoch 722/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0065 - mse: 0.1518 - val_loss: 0.0077 - val_mse: 0.1492\n",
            "Epoch 723/800\n",
            "2/2 [==============================] - 1s 257ms/step - loss: 0.0065 - mse: 0.1518 - val_loss: 0.0077 - val_mse: 0.1491\n",
            "Epoch 724/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0065 - mse: 0.1519 - val_loss: 0.0077 - val_mse: 0.1490\n",
            "Epoch 725/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0065 - mse: 0.1518 - val_loss: 0.0077 - val_mse: 0.1489\n",
            "Epoch 726/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0065 - mse: 0.1518 - val_loss: 0.0077 - val_mse: 0.1489\n",
            "Epoch 727/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0064 - mse: 0.1520 - val_loss: 0.0077 - val_mse: 0.1489\n",
            "Epoch 728/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0065 - mse: 0.1520 - val_loss: 0.0077 - val_mse: 0.1492\n",
            "Epoch 729/800\n",
            "2/2 [==============================] - 1s 279ms/step - loss: 0.0064 - mse: 0.1522 - val_loss: 0.0077 - val_mse: 0.1495\n",
            "Epoch 730/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0065 - mse: 0.1524 - val_loss: 0.0078 - val_mse: 0.1495\n",
            "Epoch 731/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0065 - mse: 0.1522 - val_loss: 0.0079 - val_mse: 0.1494\n",
            "Epoch 732/800\n",
            "2/2 [==============================] - 1s 281ms/step - loss: 0.0065 - mse: 0.1522 - val_loss: 0.0079 - val_mse: 0.1492\n",
            "Epoch 733/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0064 - mse: 0.1523 - val_loss: 0.0078 - val_mse: 0.1491\n",
            "Epoch 734/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0064 - mse: 0.1522 - val_loss: 0.0078 - val_mse: 0.1488\n",
            "Epoch 735/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0064 - mse: 0.1523 - val_loss: 0.0078 - val_mse: 0.1486\n",
            "Epoch 736/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0064 - mse: 0.1522 - val_loss: 0.0078 - val_mse: 0.1487\n",
            "Epoch 737/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0065 - mse: 0.1524 - val_loss: 0.0078 - val_mse: 0.1489\n",
            "Epoch 738/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0064 - mse: 0.1523 - val_loss: 0.0078 - val_mse: 0.1492\n",
            "Epoch 739/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0064 - mse: 0.1523 - val_loss: 0.0078 - val_mse: 0.1494\n",
            "Epoch 740/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0065 - mse: 0.1525 - val_loss: 0.0078 - val_mse: 0.1496\n",
            "Epoch 741/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0064 - mse: 0.1528 - val_loss: 0.0078 - val_mse: 0.1497\n",
            "Epoch 742/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0064 - mse: 0.1527 - val_loss: 0.0078 - val_mse: 0.1499\n",
            "Epoch 743/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0064 - mse: 0.1528 - val_loss: 0.0078 - val_mse: 0.1501\n",
            "Epoch 744/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0064 - mse: 0.1528 - val_loss: 0.0079 - val_mse: 0.1504\n",
            "Epoch 745/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0064 - mse: 0.1530 - val_loss: 0.0078 - val_mse: 0.1508\n",
            "Epoch 746/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0078 - val_mse: 0.1510\n",
            "Epoch 747/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0077 - val_mse: 0.1512\n",
            "Epoch 748/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0077 - val_mse: 0.1512\n",
            "Epoch 749/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0077 - val_mse: 0.1512\n",
            "Epoch 750/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0064 - mse: 0.1534 - val_loss: 0.0077 - val_mse: 0.1510\n",
            "Epoch 751/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0064 - mse: 0.1534 - val_loss: 0.0078 - val_mse: 0.1509\n",
            "Epoch 752/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0078 - val_mse: 0.1510\n",
            "Epoch 753/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0078 - val_mse: 0.1511\n",
            "Epoch 754/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0064 - mse: 0.1531 - val_loss: 0.0078 - val_mse: 0.1512\n",
            "Epoch 755/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0078 - val_mse: 0.1513\n",
            "Epoch 756/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0077 - val_mse: 0.1513\n",
            "Epoch 757/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0077 - val_mse: 0.1513\n",
            "Epoch 758/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0076 - val_mse: 0.1512\n",
            "Epoch 759/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0076 - val_mse: 0.1511\n",
            "Epoch 760/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0064 - mse: 0.1532 - val_loss: 0.0076 - val_mse: 0.1510\n",
            "Epoch 761/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0064 - mse: 0.1533 - val_loss: 0.0076 - val_mse: 0.1509\n",
            "Epoch 762/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0064 - mse: 0.1535 - val_loss: 0.0076 - val_mse: 0.1509\n",
            "Epoch 763/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0064 - mse: 0.1536 - val_loss: 0.0076 - val_mse: 0.1511\n",
            "Epoch 764/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0064 - mse: 0.1534 - val_loss: 0.0076 - val_mse: 0.1513\n",
            "Epoch 765/800\n",
            "2/2 [==============================] - 1s 266ms/step - loss: 0.0064 - mse: 0.1537 - val_loss: 0.0076 - val_mse: 0.1513\n",
            "Epoch 766/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0063 - mse: 0.1536 - val_loss: 0.0077 - val_mse: 0.1514\n",
            "Epoch 767/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0064 - mse: 0.1536 - val_loss: 0.0077 - val_mse: 0.1515\n",
            "Epoch 768/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0064 - mse: 0.1536 - val_loss: 0.0077 - val_mse: 0.1516\n",
            "Epoch 769/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0064 - mse: 0.1536 - val_loss: 0.0077 - val_mse: 0.1519\n",
            "Epoch 770/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0063 - mse: 0.1536 - val_loss: 0.0077 - val_mse: 0.1521\n",
            "Epoch 771/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0063 - mse: 0.1538 - val_loss: 0.0076 - val_mse: 0.1523\n",
            "Epoch 772/800\n",
            "2/2 [==============================] - 1s 271ms/step - loss: 0.0064 - mse: 0.1538 - val_loss: 0.0076 - val_mse: 0.1526\n",
            "Epoch 773/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0063 - mse: 0.1538 - val_loss: 0.0076 - val_mse: 0.1527\n",
            "Epoch 774/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0064 - mse: 0.1540 - val_loss: 0.0076 - val_mse: 0.1526\n",
            "Epoch 775/800\n",
            "2/2 [==============================] - 1s 270ms/step - loss: 0.0063 - mse: 0.1539 - val_loss: 0.0076 - val_mse: 0.1525\n",
            "Epoch 776/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0063 - mse: 0.1539 - val_loss: 0.0076 - val_mse: 0.1523\n",
            "Epoch 777/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0063 - mse: 0.1538 - val_loss: 0.0076 - val_mse: 0.1522\n",
            "Epoch 778/800\n",
            "2/2 [==============================] - 1s 267ms/step - loss: 0.0063 - mse: 0.1539 - val_loss: 0.0076 - val_mse: 0.1522\n",
            "Epoch 779/800\n",
            "2/2 [==============================] - 1s 265ms/step - loss: 0.0064 - mse: 0.1540 - val_loss: 0.0076 - val_mse: 0.1523\n",
            "Epoch 780/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0063 - mse: 0.1540 - val_loss: 0.0077 - val_mse: 0.1525\n",
            "Epoch 781/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0063 - mse: 0.1540 - val_loss: 0.0077 - val_mse: 0.1527\n",
            "Epoch 782/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0064 - mse: 0.1541 - val_loss: 0.0077 - val_mse: 0.1528\n",
            "Epoch 783/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0063 - mse: 0.1543 - val_loss: 0.0077 - val_mse: 0.1528\n",
            "Epoch 784/800\n",
            "2/2 [==============================] - 1s 262ms/step - loss: 0.0064 - mse: 0.1542 - val_loss: 0.0076 - val_mse: 0.1527\n",
            "Epoch 785/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0064 - mse: 0.1543 - val_loss: 0.0076 - val_mse: 0.1526\n",
            "Epoch 786/800\n",
            "2/2 [==============================] - 1s 272ms/step - loss: 0.0063 - mse: 0.1544 - val_loss: 0.0077 - val_mse: 0.1525\n",
            "Epoch 787/800\n",
            "2/2 [==============================] - 1s 277ms/step - loss: 0.0063 - mse: 0.1545 - val_loss: 0.0077 - val_mse: 0.1525\n",
            "Epoch 788/800\n",
            "2/2 [==============================] - 1s 269ms/step - loss: 0.0063 - mse: 0.1545 - val_loss: 0.0077 - val_mse: 0.1525\n",
            "Epoch 789/800\n",
            "2/2 [==============================] - 1s 260ms/step - loss: 0.0063 - mse: 0.1543 - val_loss: 0.0077 - val_mse: 0.1525\n",
            "Epoch 790/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0064 - mse: 0.1545 - val_loss: 0.0077 - val_mse: 0.1526\n",
            "Epoch 791/800\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 0.0064 - mse: 0.1545 - val_loss: 0.0077 - val_mse: 0.1527\n",
            "Epoch 792/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0063 - mse: 0.1544 - val_loss: 0.0076 - val_mse: 0.1528\n",
            "Epoch 793/800\n",
            "2/2 [==============================] - 1s 258ms/step - loss: 0.0063 - mse: 0.1545 - val_loss: 0.0076 - val_mse: 0.1529\n",
            "Epoch 794/800\n",
            "2/2 [==============================] - 1s 274ms/step - loss: 0.0063 - mse: 0.1544 - val_loss: 0.0077 - val_mse: 0.1529\n",
            "Epoch 795/800\n",
            "2/2 [==============================] - 1s 278ms/step - loss: 0.0063 - mse: 0.1546 - val_loss: 0.0077 - val_mse: 0.1528\n",
            "Epoch 796/800\n",
            "2/2 [==============================] - 1s 273ms/step - loss: 0.0063 - mse: 0.1546 - val_loss: 0.0077 - val_mse: 0.1526\n",
            "Epoch 797/800\n",
            "2/2 [==============================] - 1s 275ms/step - loss: 0.0063 - mse: 0.1546 - val_loss: 0.0078 - val_mse: 0.1524\n",
            "Epoch 798/800\n",
            "2/2 [==============================] - 1s 268ms/step - loss: 0.0063 - mse: 0.1547 - val_loss: 0.0078 - val_mse: 0.1521\n",
            "Epoch 799/800\n",
            "2/2 [==============================] - 1s 276ms/step - loss: 0.0063 - mse: 0.1548 - val_loss: 0.0078 - val_mse: 0.1520\n",
            "Epoch 800/800\n",
            "2/2 [==============================] - 1s 264ms/step - loss: 0.0063 - mse: 0.1549 - val_loss: 0.0078 - val_mse: 0.1519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFYcHgzUw29q",
        "colab_type": "text"
      },
      "source": [
        "## **Encoder Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngrjxagbw7O-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "7e934395-ffed-4d32-ca6a-1a8f0cc4e270"
      },
      "source": [
        "#### Model encoder building\n",
        "\n",
        "Model_encoder = Model(encoder_input, [encoder_value2, encoder_key2, encoder_h, encoder_c, encoder_bilstm_output])\n",
        "Model_encoder.save(\"Model_encoder.hdf5\")\n",
        "\n",
        "print(\"summary of encoder model:\",Model_encoder.summary())"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, None, 33)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_7 (LSTM)                   [(None, None, 32), ( 8448        input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_18 (Dense)                (None, None, 32)     1056        lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, None, 32)     1056        lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_20 (Dense)                (None, None, 32)     1056        lstm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_4 (Attention)         (None, None, 32)     1           dense_18[0][0]                   \n",
            "                                                                 dense_19[0][0]                   \n",
            "                                                                 dense_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, None, 32)     0           attention_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, None, 32)     1056        dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, None, 32)     128         dense_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_22 (Dense)                (None, None, 32)     1056        batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_23 (Dense)                (None, None, 32)     1056        batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, None, 64)     16896       input_11[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 31,809\n",
            "Trainable params: 31,745\n",
            "Non-trainable params: 64\n",
            "__________________________________________________________________________________________________\n",
            "summary of encoder model: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxSEB8wUxAKE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## **Decoder Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTy7OcluxEkW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "a141dc72-f3e5-43fd-89a4-3d493b990bff"
      },
      "source": [
        "\n",
        "#### Model decoder building\n",
        "\n",
        "# initial states come from encoder\n",
        "h_input = Input(shape=(lstm_hidden_dim,))\n",
        "c_input = Input(shape=(lstm_hidden_dim,))\n",
        "\n",
        "# Define input of Decoder Model\n",
        "\n",
        "# decoder input\n",
        "decoder_input = Input(shape = (1, train_decoder_input.shape[2]))\n",
        "\n",
        "# encoder key and value input for Multi-head attention\n",
        "value_input = Input(shape=(None, lstm_hidden_dim))\n",
        "key_input = Input(shape=(None, lstm_hidden_dim))\n",
        "\n",
        "bilstm_input = Input(shape=(1, 2*bi_lstm_hidden_dim))\n",
        "\n",
        "# layers in train model of decoder blocks transform into decoder model\n",
        "decoder_lstm_query, decoder_h, decoder_c = lstm_decoder(decoder_input, initial_state=[h_input, c_input])\n",
        "\n",
        "if whether_dense_decoder:\n",
        "  if whether_layer_normal:\n",
        "    decoder_lstm_query_copy = dense_decoder_relu(decoder_lstm_query)\n",
        "    decoder_lstm_query = LayerNormalization3(decoder_lstm_query_copy + decoder_lstm_query)\n",
        "    if whether_batch_normal:\n",
        "      decoder_lstm_query = BatchNormalization3(decoder_lstm_query)\n",
        "  else:\n",
        "    decoder_lstm_query = dense_decoder_relu(decoder_lstm_query)\n",
        "    if whether_batch_normal:\n",
        "      decoder_lstm_query = BatchNormalization3(decoder_lstm_query)\n",
        "\n",
        "attention_output = Mutihead_attention2([decoder_lstm_query, value_input, key_input])\n",
        "concat_output = concat_layer([bilstm_input, attention_output, decoder_lstm_query])\n",
        "\n",
        "if whether_concate_dense:\n",
        "  concat_output = Concate_dense(concat_output)\n",
        "  if whether_batch_normal:\n",
        "    concat_output = BatchNormalization4(concat_output)\n",
        "\n",
        "decoder_output = dense_decoder(concat_output)\n",
        "Model_decoder = Model([decoder_input, h_input, c_input, value_input, key_input, bilstm_input], [decoder_output, decoder_h, decoder_c])\n",
        "\n",
        "Model_decoder.save(\"Model_decoder.hdf5\")\n",
        "\n",
        "print(\"summary of decoder model:\",Model_decoder.summary())"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_15 (InputLayer)           [(None, 1, 32)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_13 (InputLayer)           [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           [(None, 32)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_8 (LSTM)                   multiple             8320        input_15[0][0]                   \n",
            "                                                                 input_13[0][0]                   \n",
            "                                                                 input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_24 (Dense)                multiple             1056        lstm_8[1][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor multiple             128         dense_24[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_16 (InputLayer)           [(None, None, 32)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_17 (InputLayer)           [(None, None, 32)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_18 (InputLayer)           [(None, 1, 64)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "attention_5 (Attention)         multiple             1           batch_normalization_7[1][0]      \n",
            "                                                                 input_16[0][0]                   \n",
            "                                                                 input_17[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     multiple             0           input_18[0][0]                   \n",
            "                                                                 attention_5[1][0]                \n",
            "                                                                 batch_normalization_7[1][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                multiple             8256        concatenate_2[1][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor multiple             256         dense_25[1][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                multiple             195         batch_normalization_8[1][0]      \n",
            "==================================================================================================\n",
            "Total params: 18,212\n",
            "Trainable params: 18,020\n",
            "Non-trainable params: 192\n",
            "__________________________________________________________________________________________________\n",
            "summary of decoder model: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF5QZ2vxxJWI",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKrhP38hT4lQ",
        "colab_type": "text"
      },
      "source": [
        "## **First Evaluation** - Visualisation of fitting process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqhQwlreUC66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "0bc6cfce-a939-4721-c2ec-eb12333c2117"
      },
      "source": [
        "\n",
        "import matplotlib.pylab as plt\n",
        "plt.plot(list(range(1,epoch+1)), history.history['loss'], label = 'train')\n",
        "plt.plot(list(range(1,epoch+1)), history.history['val_loss'], label = 'validation')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.title(\"loss among epochs\")\n",
        "print(\"minimum of train loss:\", min(history.history['loss']))\n",
        "print(\"minimum of validation loss:\", min(history.history['val_loss']))\n",
        "print(\"The final epoch train loss:\", history.history['loss'][-1])\n",
        "print(\"The final epoch validation loss:\", history.history['val_loss'][-1])\n",
        "print(\"The final epoch train Mean square error:\", history.history['mse'][-1])\n",
        "print(\"The final epoch validation Mean square error:\", history.history['val_mse'][-1])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "minimum of train loss: 0.0062849391251802444\n",
            "minimum of validation loss: 0.0076039680279791355\n",
            "The final epoch train loss: 0.0062981401570141315\n",
            "The final epoch validation loss: 0.007809330243617296\n",
            "The final epoch train Mean square error: 0.1548958718776703\n",
            "The final epoch validation Mean square error: 0.15187518298625946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNG98lfj-HII",
        "colab_type": "text"
      },
      "source": [
        "## **Second Evaluation** - Validation set prediction examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shd1lzhB-L5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "69d1268b-3c58-416c-c0d1-78d85e1f9a78"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# accuracy\n",
        "true_predict = 0\n",
        "false_predict = 0\n",
        "\n",
        "# F1 score\n",
        "gold_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# predictions on validation set step by step\n",
        "for i in range(len(embedding_val)):\n",
        "\n",
        "  # data generated from encoder\n",
        "  data = embedding_val[i].reshape(1, embedding_val[i].shape[0], embedding_val[i].shape[1])\n",
        "  [value_out, key_out, h, c, encoder_bilstm_out] = Model_encoder.predict(data)\n",
        "\n",
        "  # the first decoder input\n",
        "  decoder_in = BOS_vector.reshape(1,1,BOS_vector.shape[0])\n",
        "  predictions = []\n",
        "\n",
        "  # encoder value and key output as decoder value and key input\n",
        "  value_in = value_out\n",
        "  key_in = key_out\n",
        "\n",
        "  # Name entity set generation step by step, through decoder\n",
        "  for j in range(len(data_val[i][1])):\n",
        "    bilstm_in = encoder_bilstm_out[0][j].reshape(1, 1, encoder_bilstm_out[0][j].shape[0])\n",
        "    [decoder_out, h, c] = Model_decoder.predict([decoder_in, h, c, value_in, key_in, bilstm_in])\n",
        "    prediction = np.argmax(decoder_out)\n",
        "    prediction = int_as_labels[prediction]\n",
        "    predictions.append(prediction)\n",
        "    if(data_val[i][1][j] == prediction):\n",
        "      true_predict += 1\n",
        "    else:\n",
        "      false_predict += 1\n",
        "    decoder_in = Nameentity_model[prediction]\n",
        "    decoder_in = decoder_in.reshape(1,1,decoder_in.shape[0])\n",
        "  predicted_labels += predictions\n",
        "  gold_labels += data_val[i][1]\n",
        "  if(i % 10 == 9):\n",
        "    accuracy = (true_predict)/(true_predict + false_predict)\n",
        "    print('\\n',i,\"/\", len(embedding_val), \"validation data accuracy:\", accuracy)\n",
        "    print(\"Input: \", \" \".join(data_val[i][0]))\n",
        "    print(\"Gold Name Entity:\", \" \".join(data_val[i][1]))\n",
        "    print(\"Predicted Name Entity:\", \" \".join(predictions))\n",
        "accuracy = (true_predict)/(true_predict + false_predict)\n",
        "print(\"\\nOverall accuracy (micro average f1 score):\", accuracy)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 9 / 85 validation data accuracy: 0.9955947136563876\n",
            "Input:  Subjects were assigned to a no-accountability condition ( they learned that all of their responses would be anonymous ) , a preexposure-accountability condition ( they learned of the need to justify their responses before seeing the test-takers ' PRF responses ) , and a postexposure-accountability condition ( they learned of the need to justify their responses after seeing the test-takers ' PRF responses ) .\n",
            "Gold Name Entity: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "Predicted Name Entity: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\n",
            " 19 / 85 validation data accuracy: 0.979253112033195\n",
            "Input:  Insulin and insulin monoiodinated in tyrosine A14 , A19 , B16 and B26 can be separated using reversed-phase high-performance liquid chromatography on a number of C18 columns eluted with acetonitrile containing triethylammonium phosphate or acetate buffers .\n",
            "Gold Name Entity: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "Predicted Name Entity: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\n",
            " 29 / 85 validation data accuracy: 0.9762962962962963\n",
            "Input:  The expression of pro-inflammatory cytokine interleukin-1β was also increased in DP , but not PDL cells .\n",
            "Gold Name Entity: O O O O O O O O O O O O O O O O O\n",
            "Predicted Name Entity: O O O O O O O O O O O O O O O O O\n",
            "\n",
            " 39 / 85 validation data accuracy: 0.9713024282560706\n",
            "Input:  By understanding death with an LVAD in place , clinicians are in a better position to educate patients and caregivers about what to expect and provide to support tailored to patient and caregiver needs .\n",
            "Gold Name Entity: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "Predicted Name Entity: O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O\n",
            "\n",
            " 49 / 85 validation data accuracy: 0.9684115523465704\n",
            "Input:  The aim of this study was to evaluate the extent of residual primary disease and local recurrence as well as the outcome of salvage surgery after CRT for anal carcinoma in HIV-positive individuals .\n",
            "Gold Name Entity: O O O O O O O O O O O O B-indications I-indications O O O O O O O O O O O O O O B-indications I-indications O O O O\n",
            "Predicted Name Entity: O O O O O O O O O O O B-indications I-indications I-indications O O O O O O O O O B-indications I-indications O O O B-indications I-indications O B-indications I-indications O\n",
            "\n",
            " 59 / 85 validation data accuracy: 0.9706546275395034\n",
            "Input:  Only 38 cases have been reported worldwide .\n",
            "Gold Name Entity: O O O O O O O O\n",
            "Predicted Name Entity: O O O O O O O O\n",
            "\n",
            " 69 / 85 validation data accuracy: 0.9649661954517517\n",
            "Input:  As a result , this muscle belly was innervated partially in a non-segmental manner .\n",
            "Gold Name Entity: O O O O O O O O O O O O O O O\n",
            "Predicted Name Entity: O O O O O O O O O O O O O O O\n",
            "\n",
            " 79 / 85 validation data accuracy: 0.967741935483871\n",
            "Input:  Deletion mutagenesis and firefly luciferase reporter gene assay demonstrated the essential contribution of Sp1 on transcriptional activation of the Ki-67 gene .\n",
            "Gold Name Entity: O O O O O O O O O O O O O O O O O O O O O O\n",
            "Predicted Name Entity: O O O O O O O O O O O O O O O O O O O O O O\n",
            "\n",
            "Overall accuracy (micro average f1 score): 0.9692937563971341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5pv8SfBxlqO",
        "colab_type": "text"
      },
      "source": [
        "## **Third Evaluation** - Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZgoN--1ce1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "fedf677c-50ef-4203-b384-1442b939ff5d"
      },
      "source": [
        "# Reference https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "evaluation_metrics = classification_report(gold_labels, predicted_labels)\n",
        "print(evaluation_metrics)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "B-indications       0.30      0.10      0.15        31\n",
            "I-indications       0.38      0.27      0.32        22\n",
            "            O       0.98      0.99      0.98      1901\n",
            "\n",
            "     accuracy                           0.97      1954\n",
            "    macro avg       0.55      0.45      0.48      1954\n",
            " weighted avg       0.96      0.97      0.96      1954\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyrP_JMHqvNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "07e3537f-3b0d-4631-e3a9-06023d6a2ed2"
      },
      "source": [
        "# detailed F1 score\n",
        "# Reference https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
        "\n",
        "micro_f1 = f1_score(gold_labels, predicted_labels, average=\"micro\")\n",
        "macro_f1 = f1_score(gold_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(gold_labels, predicted_labels, average=\"weighted\")\n",
        "\n",
        "print(\"micro average F1 score on Validation set:\", micro_f1)\n",
        "print(\"macro average F1 score on Validation set:\", macro_f1)\n",
        "print(\"weighted average F1 score on Validation set:\", weighted_f1)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "micro average F1 score on Validation set: 0.9692937563971341\n",
            "macro average F1 score on Validation set: 0.48224073806489737\n",
            "weighted average F1 score on Validation set: 0.9637625238209377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA07I4VDdyr5",
        "colab_type": "text"
      },
      "source": [
        "# **Test data prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LNixaGPdvtA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "080fa39f-f571-4ef8-b800-5658e64d8710"
      },
      "source": [
        "# test data prediction is similar to validation data prediction\n",
        "import pandas as pd\n",
        "predictions_list = []\n",
        "predictions = []\n",
        "\n",
        "# predict test data and record to csv doc\n",
        "for i in range(len(embedding_test)):\n",
        "    data = embedding_test[i].reshape(1, embedding_test[i].shape[0], embedding_test[i].shape[1])\n",
        "    [value_out, key_out, h, c, encoder_bilstm_out] = Model_encoder.predict(data)\n",
        "    decoder_in = BOS_vector.reshape(1,1,BOS_vector.shape[0])\n",
        "    value_in = value_out\n",
        "    key_in = key_out\n",
        "\n",
        "    for j in range(len(data_test[i][0])):\n",
        "        bilstm_in = encoder_bilstm_out[0][j].reshape(1, 1, encoder_bilstm_out[0][j].shape[0])\n",
        "        [decoder_out, h, c] = Model_decoder.predict([decoder_in, h, c, value_in, key_in, bilstm_in])\n",
        "        prediction = np.argmax(decoder_out)\n",
        "        prediction = int_as_labels[prediction]\n",
        "        predictions.append(prediction)\n",
        "        decoder_in = Nameentity_model[prediction]\n",
        "        decoder_in = decoder_in.reshape(1,1,decoder_in.shape[0])\n",
        "    predictions_list.append(predictions)\n",
        "    predictions = []\n",
        "\n",
        "dataframe = {\"Prediction\": predictions_list}\n",
        "dataframe = pd.DataFrame.from_dict(dataframe)\n",
        "dataframe.to_csv(\"prediction_result.csv\")"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm0oZ2RCOkRZ",
        "colab_type": "text"
      },
      "source": [
        "# **Reference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK1oyNhzOjpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word2Vec based on given words. Reference: https://radimrehurek.com/gensim/models/word2vec.html\n",
        "# TFIDF Reference: http://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/\n",
        "# TFIDF in vectors Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "# Word2Vec Model for Pos-tagging based on generated Pos-tagging. Reference: https://radimrehurek.com/gensim/models/word2vec.html\n",
        "# Seq2seq Reference: https://keras.io/examples/lstm_seq2seq/\n",
        "# Dot product Attention layer Reference: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention\n",
        "# Muti-head Attention structure Reference: https://www.tensorflow.org/tutorials/text/transformer\n",
        "# Tensorflow with GPU for colab Reference: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=Y04m-jvKRDsJ\n",
        "# Classification report Reference https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "# F1 score Reference https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
      ],
      "execution_count": 128,
      "outputs": []
    }
  ]
}